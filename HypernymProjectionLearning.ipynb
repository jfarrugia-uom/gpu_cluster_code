{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Word2Vec binary word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = 'GoogleNews-vectors-negative300.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to text version\n",
    "#model.save_word2vec_format('GoogleNews-vectors-negative300.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(w2v, binary=False)\n",
    "# pre-compute L2 norms of vectors\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_subsumptions(filename):\n",
    "    subsumptions = []\n",
    "\n",
    "    with codecs.open(filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            subsumptions.append((row[0], row[1]))\n",
    "\n",
    "    return subsumptions\n",
    "\n",
    "def read_synonyms(filename):\n",
    "    synonyms = defaultdict(lambda: list())\n",
    "\n",
    "    with codecs.open(filename,encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "        for row in reader:\n",
    "            for word in row[1].split(','):\n",
    "                synonyms[row[0]].append(word)\n",
    "    \n",
    "    synonyms.default_factory = None\n",
    "    return synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_subs = read_subsumptions('subsumptions-train.txt.orig')\n",
    "test_subs = read_subsumptions('subsumptions-test.txt.orig')\n",
    "valid_subs = read_subsumptions('subsumptions-validation.txt.orig')\n",
    "\n",
    "synonyms = read_synonyms('synonyms.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct pre-trained word embeddings dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# eliminate training tuples for which no embedding exists\n",
    "from collections import Counter\n",
    "\n",
    "def get_terms_having_vectors(dataset):    \n",
    "    query, hyper = \\\n",
    "    zip(*[(q,h) for q, h in dataset \n",
    "          if q in model and h in model])\n",
    "    \n",
    "    return list(query), list(hyper)\n",
    "    \n",
    "train_query, train_hyper = get_terms_having_vectors(train_subs)\n",
    "test_query, test_hyper = get_terms_having_vectors(test_subs)\n",
    "\n",
    "assert len(train_query) == len(train_hyper)\n",
    "assert len(test_query) == len(test_hyper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove OOV from synonym list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k, v in list(synonyms.items()):\n",
    "    if k not in model:\n",
    "        synonyms.pop(k)\n",
    "    else:\n",
    "        for word in v:\n",
    "            if word not in model:\n",
    "                v.remove(word)\n",
    "    \n",
    "# flatten list of synonyms    \n",
    "syns = [word for v in synonyms.values() for word in v]    \n",
    "\n",
    "# confirm that all words in synonym vocab have embeddings representation\n",
    "assert len(list(filter(lambda x: x in model, syns)))==len(syns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Define class Data which encapsulates all the bits and pieces we require for training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Data class that encapsulates all word-based data I need to train the various algorithms\n",
    "# We assume that we have all pre-filtered any words that don't feature in the embeddings\n",
    "class Data:\n",
    "    def __init__(self, train_query, train_hyper, test_query, test_hyper, synonyms, embeddings):\n",
    "        # construct vocab made up from term and hypernyms \n",
    "        # we will choose negative samples from this vocab after exhausting\n",
    "        # the synonyms\n",
    "        self.neg_vocab = set(train_hyper + test_hyper)\n",
    "        \n",
    "        # encapsulate input variables so that all the data can be passed via class instance reference\n",
    "        self.train_query = train_query\n",
    "        self.train_hyper = train_hyper\n",
    "        self.test_query = test_query\n",
    "        self.test_hyper = test_hyper\n",
    "        self.synonyms = synonyms\n",
    "        \n",
    "        # calculate size of term and hypernym dataset (train + test)\n",
    "        n_hyponyms = len(set(train_query + test_query + syns))\n",
    "        # hypernyms will be introduced in the model as either training,\n",
    "        # gold positives, test gold positives (when evaluation) or\n",
    "        # negative synonyms.\n",
    "        n_hypernyms = len(set(train_hyper + test_hyper))\n",
    "\n",
    "        # determine dimensionality of embeddings\n",
    "        self.embeddings_dim = embeddings['animal'].shape[0]\n",
    "        # intialise and fit tokenizer\n",
    "        self.tokenizer = Tokenizer(num_words = n_hyponyms + n_hypernyms + 1, filters='!\"#$%&()*+,-./:;<=>?@[\\]^`{|}~)')\n",
    "        self.tokenizer.fit_on_texts(train_query + train_hyper + test_query + test_hyper + syns)\n",
    "        \n",
    "        # construct embedding_matrix\n",
    "        self.embedding_matrix = np.zeros((len(self.tokenizer.word_index)+1, self.embeddings_dim), dtype='float32')\n",
    "\n",
    "        for word, i in self.tokenizer.word_index.items():\n",
    "            if i < len(self.tokenizer.word_index) + 1:\n",
    "                embedding_vector = embeddings[word]\n",
    "                if embedding_vector is not None:\n",
    "                    # normalise vector (already normalised)\n",
    "                    #embedding_vector /= np.linalg.norm(embedding_vector)\n",
    "                    self.embedding_matrix[i,:] = embedding_vector  \n",
    "        # confirm shape\n",
    "        assert self.embedding_matrix.shape == (len(self.tokenizer.word_index)+1, self.embeddings_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = Data(train_query, train_hyper, test_query, test_hyper, synonyms, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Negative Sampling Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first exhaust synonyms;\n",
    "# find the rest by drawing random terms from neg_vocab;\n",
    "# however, make sure that chosen words are not valid hypernyms;\n",
    "# finally, tokenise back to ids;    \n",
    "\n",
    "\n",
    "# positive_sample and terms both expect tuples where positive_sample = (query, hyper)\n",
    "# and terms = (all_query_terms, all_hyper_terms)\n",
    "def get_negative_words(positive_sample, word_hypernyms, data, sample_size=5):\n",
    "    neg_samples = []\n",
    "    # we need to make a copy of the synonym list\n",
    "    # synonmys will form part of out negative examples\n",
    "    if positive_sample[0] in data.synonyms:\n",
    "        neg_samples = list(synonyms[positive_sample[0]])        \n",
    "    \n",
    "    # there might not be enough; compound with random words\n",
    "    if len(neg_samples) >= sample_size:\n",
    "        # jumble negative sample indices        \n",
    "        neg_samples = np.random.choice(neg_samples, sample_size, replace=False)\n",
    "    else:\n",
    "        # get current sample's hypernyms\n",
    "        positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "        \n",
    "        # eliminate correct hypernyms from neg_vocab\n",
    "        word_choice = [nv for nv in data.neg_vocab if nv not in positive_hypernyms and nv not in neg_samples]        \n",
    "        # choose m - len(neg_samples)\n",
    "        neg_samples.extend(np.random.choice(word_choice, (sample_size-len(neg_samples))).tolist())\n",
    "            \n",
    "    return neg_samples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_negative_words(positive_sample, word_hypernyms, data, sample_size=5):\n",
    "    neg_samples = []\n",
    "    \n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    sample_space = filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys())\n",
    "    neg_samples = np.random.choice(list(sample_space), sample_size, replace=False)\n",
    "    \n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find most similar words to given hypernym which is not valid hypernym of word\n",
    "def get_similar_hypernyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    word = data.tokenizer.word_index[positive_sample[1]]\n",
    "    candidate_words = list(filter(lambda w: w != word, data.tokenizer.index_word.keys()))\n",
    "    sims = list(map(lambda c: np.dot(data.embedding_matrix[c], data.embedding_matrix[word]), candidate_words))\n",
    "\n",
    "    # get 30 most similar words to hypernyms\n",
    "    most_sim_idx = np.argsort(sims)[::-1][:30]    \n",
    "    similar_hypernyms = [data.tokenizer.index_word[candidate_words[idx]] for idx in most_sim_idx]\n",
    "    \n",
    "    # make sure that similar words are not actual hypernyms    \n",
    "    positive_hypernym = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    return list(filter(lambda x: x not in positive_hypernym, similar_hypernyms))[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def get_similar_hyponyms(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])        \n",
    "\n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = list(filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys()))\n",
    "    \n",
    "    hypo_sims = list(map(lambda c: np.dot(        \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[0]]]), candidate_words))\n",
    "    \n",
    "    most_sim_idx = np.argsort(hypo_sims)[::-1][:sample_size]\n",
    "    return list(map(lambda i: candidate_words[i], most_sim_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get one similar hypernym and random words\n",
    "def mix_sim_hyper_random(positive_sample, word_hypernyms, data, sample_size=5):    \n",
    "    # init neg_samples\n",
    "    neg_samples = []\n",
    "    \n",
    "    # get current sample's hypernyms\n",
    "    positive_hypernyms = word_hypernyms[positive_sample[0]]\n",
    "    \n",
    "    # add query term to hypernym list\n",
    "    positive_hypernyms.append(positive_sample[0])    \n",
    "    \n",
    "    # get candidate words - all vocab except hypernyms of current word and current word\n",
    "    candidate_words = list(filter(lambda w: w not in positive_hypernyms, data.tokenizer.word_index.keys()))\n",
    "    \n",
    "    \n",
    "    # find similarity of all candidate words w.r.t. current hypernym\n",
    "    hyper_sims = list(map(lambda c: np.dot(\n",
    "                            data.embedding_matrix[data.tokenizer.word_index[c]], \n",
    "                            data.embedding_matrix[data.tokenizer.word_index[positive_sample[1]]]), candidate_words))\n",
    "\n",
    "    # get most similar word to hypernym which is not hypernym\n",
    "    most_sim_idx = np.argsort(hyper_sims)[::-1][0]    \n",
    "    # append most similar hypernym to negative samples\n",
    "    neg_samples.append(candidate_words[most_sim_idx])\n",
    "        \n",
    "    if len(neg_samples) < sample_size:\n",
    "        neg_samples.extend(get_negative_words(positive_sample, word_hypernyms, data, sample_size=sample_size-1))\n",
    "    \n",
    "    return neg_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create list of tuples where every element follows (word, negative_word)\n",
    "def get_negative_tuples(terms, data, negative_words_lambda, sample_size):\n",
    "    # convert terms to dictionary\n",
    "    input_query, input_hyper = terms\n",
    "    unq_input_query = sorted(list(set(input_query)))\n",
    "    \n",
    "    word_hypernyms = {}\n",
    "    for w in unq_input_query:        \n",
    "        word_hypernyms[w] = [h for q, h in zip(input_query, input_hyper) if q == w]\n",
    "        \n",
    "            \n",
    "    negative_tuples = []    \n",
    "    for words in zip(*terms):\n",
    "        negatives = negative_words_lambda(words, word_hypernyms, data, sample_size)\n",
    "        negative_tuples.extend(\n",
    "                [(words, n) for n in negatives]\n",
    "        )    \n",
    "    return negative_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('vision', 'experience'), 'bookstore'),\n",
       " (('vision', 'experience'), 'content'),\n",
       " (('vision', 'experience'), 'mold'),\n",
       " (('vision', 'experience'), 'missouri'),\n",
       " (('vision', 'experience'), 'man'),\n",
       " (('lime', 'citrus'), 'tobacco'),\n",
       " (('lime', 'citrus'), 'useful'),\n",
       " (('lime', 'citrus'), 'yew'),\n",
       " (('lime', 'citrus'), 'cyclical'),\n",
       " (('lime', 'citrus'), 'placental'),\n",
       " (('lime', 'plant'), 'dwell'),\n",
       " (('lime', 'plant'), 'look'),\n",
       " (('lime', 'plant'), 'player'),\n",
       " (('lime', 'plant'), 'company'),\n",
       " (('lime', 'plant'), 'rest'),\n",
       " (('lime', 'tree'), 'machine'),\n",
       " (('lime', 'tree'), 'separation'),\n",
       " (('lime', 'tree'), 'fold'),\n",
       " (('lime', 'tree'), 'handcart'),\n",
       " (('lime', 'tree'), 'mechanical'),\n",
       " (('lime', 'food'), 'grind'),\n",
       " (('lime', 'food'), 'ideology'),\n",
       " (('lime', 'food'), 'receive'),\n",
       " (('lime', 'food'), 'abalone'),\n",
       " (('lime', 'food'), 'hippopotamus'),\n",
       " (('lime', 'produce'), 'seaplane'),\n",
       " (('lime', 'produce'), 'exchange'),\n",
       " (('lime', 'produce'), 'sunlight'),\n",
       " (('lime', 'produce'), 'devolve'),\n",
       " (('lime', 'produce'), 'cell'),\n",
       " (('lime', 'fruit'), 'need'),\n",
       " (('lime', 'fruit'), 'gauge'),\n",
       " (('lime', 'fruit'), 'asparagus'),\n",
       " (('lime', 'fruit'), 'miss'),\n",
       " (('lime', 'fruit'), 'close'),\n",
       " (('train', 'artefact'), 'backtrack'),\n",
       " (('train', 'artefact'), 'lion'),\n",
       " (('train', 'artefact'), 'unhappiness'),\n",
       " (('train', 'artefact'), 'wildflower'),\n",
       " (('train', 'artefact'), 'satellite'),\n",
       " (('train', 'conveyance'), 'add'),\n",
       " (('train', 'conveyance'), 'feature'),\n",
       " (('train', 'conveyance'), 'normative'),\n",
       " (('train', 'conveyance'), 'mix'),\n",
       " (('train', 'conveyance'), 'paid'),\n",
       " (('train', 'transport'), 'pear'),\n",
       " (('train', 'transport'), 'admire'),\n",
       " (('train', 'transport'), 'uncertain'),\n",
       " (('train', 'transport'), 'bee'),\n",
       " (('train', 'transport'), 'balance'),\n",
       " (('train', 'vehicle'), 'frog'),\n",
       " (('train', 'vehicle'), 'trim'),\n",
       " (('train', 'vehicle'), 'tern'),\n",
       " (('train', 'vehicle'), 'cap'),\n",
       " (('train', 'vehicle'), 'professor'),\n",
       " (('train', 'artifact'), 'oriole'),\n",
       " (('train', 'artifact'), 'helm'),\n",
       " (('train', 'artifact'), 'experience'),\n",
       " (('train', 'artifact'), 'accommodation'),\n",
       " (('train', 'artifact'), 'toilet'),\n",
       " (('train', 'practice'), 'decrease'),\n",
       " (('train', 'practice'), 'birch'),\n",
       " (('train', 'practice'), 'pony'),\n",
       " (('train', 'practice'), 'all'),\n",
       " (('train', 'practice'), 'kalashnikov'),\n",
       " (('train', 'transportation'), 'island'),\n",
       " (('train', 'transportation'), 'orangutan'),\n",
       " (('train', 'transportation'), 'family'),\n",
       " (('train', 'transportation'), 'develop'),\n",
       " (('train', 'transportation'), 'cover'),\n",
       " (('train', 'learn'), 'breast'),\n",
       " (('train', 'learn'), 'pass'),\n",
       " (('train', 'learn'), 'pluck'),\n",
       " (('train', 'learn'), 'cardamom'),\n",
       " (('train', 'learn'), 'silverfish'),\n",
       " (('train', 'teach'), 'direction'),\n",
       " (('train', 'teach'), 'chinchilla'),\n",
       " (('train', 'teach'), 'everything'),\n",
       " (('train', 'teach'), 'spinach'),\n",
       " (('train', 'teach'), 'hummingbird'),\n",
       " (('hawthorn', 'shrub'), 'tapeworm'),\n",
       " (('hawthorn', 'shrub'), 'dragonfly'),\n",
       " (('hawthorn', 'shrub'), 'stretch'),\n",
       " (('hawthorn', 'shrub'), 'soup'),\n",
       " (('hawthorn', 'shrub'), 'succeed'),\n",
       " (('hawthorn', 'plant'), 'manufacture'),\n",
       " (('hawthorn', 'plant'), 'blink'),\n",
       " (('hawthorn', 'plant'), 'humility'),\n",
       " (('hawthorn', 'plant'), 'rack'),\n",
       " (('hawthorn', 'plant'), 'hydrangea'),\n",
       " (('rosemary', 'herb'), 'disgust'),\n",
       " (('rosemary', 'herb'), 'funded'),\n",
       " (('rosemary', 'herb'), 'touch'),\n",
       " (('rosemary', 'herb'), 'alto'),\n",
       " (('rosemary', 'herb'), 'arum'),\n",
       " (('rosemary', 'plant'), 'cormorant'),\n",
       " (('rosemary', 'plant'), 'hilly'),\n",
       " (('rosemary', 'plant'), 'conveyance'),\n",
       " (('rosemary', 'plant'), 'choke'),\n",
       " (('rosemary', 'plant'), 'motorboat'),\n",
       " (('crab', 'arthropod'), 'success'),\n",
       " (('crab', 'arthropod'), 'pant'),\n",
       " (('crab', 'arthropod'), 'wren'),\n",
       " (('crab', 'arthropod'), 'hop'),\n",
       " (('crab', 'arthropod'), 'plum'),\n",
       " (('crab', 'invertebrate'), 'gibbon'),\n",
       " (('crab', 'invertebrate'), 'toad'),\n",
       " (('crab', 'invertebrate'), 'wardrobe'),\n",
       " (('crab', 'invertebrate'), 'compact'),\n",
       " (('crab', 'invertebrate'), 'cedar'),\n",
       " (('crab', 'animal'), 'dart'),\n",
       " (('crab', 'animal'), 'example'),\n",
       " (('crab', 'animal'), 'limit'),\n",
       " (('crab', 'animal'), 'backtrack'),\n",
       " (('crab', 'animal'), 'calendula'),\n",
       " (('crab', 'crustacean'), 'mall'),\n",
       " (('crab', 'crustacean'), 'determined'),\n",
       " (('crab', 'crustacean'), 'razor'),\n",
       " (('crab', 'crustacean'), 'tarsier'),\n",
       " (('crab', 'crustacean'), 'kazoo'),\n",
       " (('crab', 'shellfish'), 'submersible'),\n",
       " (('crab', 'shellfish'), 'bomber'),\n",
       " (('crab', 'shellfish'), 'permanent'),\n",
       " (('crab', 'shellfish'), 'exhale'),\n",
       " (('crab', 'shellfish'), 'undemocratic'),\n",
       " (('funded', 'monetary'), 'marten'),\n",
       " (('funded', 'monetary'), 'mammal'),\n",
       " (('funded', 'monetary'), 'mangrove'),\n",
       " (('funded', 'monetary'), 'wrench'),\n",
       " (('funded', 'monetary'), 'complex'),\n",
       " (('funded', 'paid'), 'livestock'),\n",
       " (('funded', 'paid'), 'fitness'),\n",
       " (('funded', 'paid'), 'unprepared'),\n",
       " (('funded', 'paid'), 'measured'),\n",
       " (('funded', 'paid'), 'unhealthy'),\n",
       " (('funded', 'abundant'), 'apparel'),\n",
       " (('funded', 'abundant'), 'exercise'),\n",
       " (('funded', 'abundant'), 'hickory'),\n",
       " (('funded', 'abundant'), 'outside'),\n",
       " (('funded', 'abundant'), 'seedling'),\n",
       " (('funded', 'supported'), 'story'),\n",
       " (('funded', 'supported'), 'element'),\n",
       " (('funded', 'supported'), 'achillea'),\n",
       " (('funded', 'supported'), 'individual'),\n",
       " (('funded', 'supported'), 'mallet'),\n",
       " (('funded', 'resourced'), 'delight'),\n",
       " (('funded', 'resourced'), 'run'),\n",
       " (('funded', 'resourced'), 'unchanged'),\n",
       " (('funded', 'resourced'), 'pig'),\n",
       " (('funded', 'resourced'), 'secure'),\n",
       " (('carabao', 'chordate'), 'lime'),\n",
       " (('carabao', 'chordate'), 'copy'),\n",
       " (('carabao', 'chordate'), 'unusual'),\n",
       " (('carabao', 'chordate'), 'seasoning'),\n",
       " (('carabao', 'chordate'), 'backpack'),\n",
       " (('carabao', 'vertebrate'), 'walnut'),\n",
       " (('carabao', 'vertebrate'), 'tavern'),\n",
       " (('carabao', 'vertebrate'), 'stocking'),\n",
       " (('carabao', 'vertebrate'), 'molecule'),\n",
       " (('carabao', 'vertebrate'), 'stem'),\n",
       " (('carabao', 'ungulate'), 'insectivore'),\n",
       " (('carabao', 'ungulate'), 'flathead'),\n",
       " (('carabao', 'ungulate'), 'hold'),\n",
       " (('carabao', 'ungulate'), 'iguana'),\n",
       " (('carabao', 'ungulate'), 'burro'),\n",
       " (('carabao', 'ruminant'), 'chain'),\n",
       " (('carabao', 'ruminant'), 'mollusk'),\n",
       " (('carabao', 'ruminant'), 'timeless'),\n",
       " (('carabao', 'ruminant'), 'tuna'),\n",
       " (('carabao', 'ruminant'), 'bitter'),\n",
       " (('carabao', 'placental'), 'reverse'),\n",
       " (('carabao', 'placental'), 'dandelion'),\n",
       " (('carabao', 'placental'), 'people'),\n",
       " (('carabao', 'placental'), 'rag'),\n",
       " (('carabao', 'placental'), 'authority'),\n",
       " (('carabao', 'mammal'), 'traffic_jam'),\n",
       " (('carabao', 'mammal'), 'atmosphere'),\n",
       " (('carabao', 'mammal'), 'all'),\n",
       " (('carabao', 'mammal'), 'millipede'),\n",
       " (('carabao', 'mammal'), 'willing'),\n",
       " (('carabao', 'animal'), 'headgear'),\n",
       " (('carabao', 'animal'), 'carrier'),\n",
       " (('carabao', 'animal'), 'clock'),\n",
       " (('carabao', 'animal'), 'aircraft'),\n",
       " (('carabao', 'animal'), 'positive'),\n",
       " (('skyline', 'horizon'), 'spice'),\n",
       " (('skyline', 'horizon'), 'darkness'),\n",
       " (('skyline', 'horizon'), 'life'),\n",
       " (('skyline', 'horizon'), 'divide'),\n",
       " (('skyline', 'horizon'), 'flow'),\n",
       " (('skyline', 'outline'), 'establishment'),\n",
       " (('skyline', 'outline'), 'nameless'),\n",
       " (('skyline', 'outline'), 'cell_phone'),\n",
       " (('skyline', 'outline'), 'yam'),\n",
       " (('skyline', 'outline'), 'adventure'),\n",
       " (('soar', 'fly'), 'interesting'),\n",
       " (('soar', 'fly'), 'creature'),\n",
       " (('soar', 'fly'), 'slice'),\n",
       " (('soar', 'fly'), 'figure'),\n",
       " (('soar', 'fly'), 'slip'),\n",
       " (('culture', 'growth'), 'broccoli'),\n",
       " (('culture', 'growth'), 'limpet'),\n",
       " (('culture', 'growth'), 'rubber'),\n",
       " (('culture', 'growth'), 'tumbleweed'),\n",
       " (('culture', 'growth'), 'springbok'),\n",
       " (('monk', 'person'), 'tamarisk'),\n",
       " (('monk', 'person'), 'hart'),\n",
       " (('monk', 'person'), 'barley'),\n",
       " (('monk', 'person'), 'excite'),\n",
       " (('monk', 'person'), 'grow'),\n",
       " (('trial', 'experiment'), 'postal'),\n",
       " (('trial', 'experiment'), 'thick'),\n",
       " (('trial', 'experiment'), 'defense'),\n",
       " (('trial', 'experiment'), 'temperature'),\n",
       " (('trial', 'experiment'), 'respiratory'),\n",
       " (('snowball', 'plaything'), 'leopard'),\n",
       " (('snowball', 'plaything'), 'dwelling'),\n",
       " (('snowball', 'plaything'), 'asia'),\n",
       " (('snowball', 'plaything'), 'aluminum'),\n",
       " (('snowball', 'plaything'), 'nice'),\n",
       " (('snowball', 'ball'), 'frost'),\n",
       " (('snowball', 'ball'), 'tamarisk'),\n",
       " (('snowball', 'ball'), 'clothes'),\n",
       " (('snowball', 'ball'), 'rose'),\n",
       " (('snowball', 'ball'), 'tern'),\n",
       " (('snowball', 'condensation'), 'classification'),\n",
       " (('snowball', 'condensation'), 'familiarize'),\n",
       " (('snowball', 'condensation'), 'strange'),\n",
       " (('snowball', 'condensation'), 'magazine'),\n",
       " (('snowball', 'condensation'), 'planarian'),\n",
       " (('snowball', 'weather'), 'ride'),\n",
       " (('snowball', 'weather'), 'louse'),\n",
       " (('snowball', 'weather'), 'bandanna'),\n",
       " (('snowball', 'weather'), 'impure'),\n",
       " (('snowball', 'weather'), 'belief'),\n",
       " (('snowball', 'sphere'), 'bindweed'),\n",
       " (('snowball', 'sphere'), 'casuarina'),\n",
       " (('snowball', 'sphere'), 'equipment'),\n",
       " (('snowball', 'sphere'), 'orchestral'),\n",
       " (('snowball', 'sphere'), 'government'),\n",
       " (('snowball', 'mass'), 'code'),\n",
       " (('snowball', 'mass'), 'clear'),\n",
       " (('snowball', 'mass'), 'protection'),\n",
       " (('snowball', 'mass'), 'nutcracker'),\n",
       " (('snowball', 'mass'), 'literature'),\n",
       " (('toilet', 'room'), 'cruel'),\n",
       " (('toilet', 'room'), 'join'),\n",
       " (('toilet', 'room'), 'keeper'),\n",
       " (('toilet', 'room'), 'sex'),\n",
       " (('toilet', 'room'), 'globe'),\n",
       " (('problem', 'question'), 'building'),\n",
       " (('problem', 'question'), 'annelid'),\n",
       " (('problem', 'question'), 'molecule'),\n",
       " (('problem', 'question'), 'separation'),\n",
       " (('problem', 'question'), 'counselor'),\n",
       " (('direct', 'order'), 'drum'),\n",
       " (('direct', 'order'), 'sharp'),\n",
       " (('direct', 'order'), 'bicycle'),\n",
       " (('direct', 'order'), 'angelica'),\n",
       " (('direct', 'order'), 'nematode'),\n",
       " (('direct', 'make'), 'chick'),\n",
       " (('direct', 'make'), 'tan'),\n",
       " (('direct', 'make'), 'chard'),\n",
       " (('direct', 'make'), 'mississippi'),\n",
       " (('direct', 'make'), 'agreement'),\n",
       " (('pulmonary', 'respiratory'), 'whale'),\n",
       " (('pulmonary', 'respiratory'), 'land'),\n",
       " (('pulmonary', 'respiratory'), 'bovine'),\n",
       " (('pulmonary', 'respiratory'), 'independent'),\n",
       " (('pulmonary', 'respiratory'), 'artist'),\n",
       " (('pulmonary', 'physical'), 'word'),\n",
       " (('pulmonary', 'physical'), 'communism'),\n",
       " (('pulmonary', 'physical'), 'chest'),\n",
       " (('pulmonary', 'physical'), 'grain'),\n",
       " (('pulmonary', 'physical'), 'shaped'),\n",
       " (('pulmonary', 'medical'), 'data'),\n",
       " (('pulmonary', 'medical'), 'whale'),\n",
       " (('pulmonary', 'medical'), 'inform'),\n",
       " (('pulmonary', 'medical'), 'border'),\n",
       " (('pulmonary', 'medical'), 'firearm'),\n",
       " (('chest', 'box'), 'songbird'),\n",
       " (('chest', 'box'), 'orbital'),\n",
       " (('chest', 'box'), 'proportionate'),\n",
       " (('chest', 'box'), 'kenaf'),\n",
       " (('chest', 'box'), 'season'),\n",
       " (('jasmine', 'shrub'), 'galax'),\n",
       " (('jasmine', 'shrub'), 'division'),\n",
       " (('jasmine', 'shrub'), 'coupe'),\n",
       " (('jasmine', 'shrub'), 'scissors'),\n",
       " (('jasmine', 'shrub'), 'deaf'),\n",
       " (('jasmine', 'plant'), 'brook'),\n",
       " (('jasmine', 'plant'), 'repeat'),\n",
       " (('jasmine', 'plant'), 'science'),\n",
       " (('jasmine', 'plant'), 'mysterious'),\n",
       " (('jasmine', 'plant'), 'magazine'),\n",
       " (('critter', 'animal'), 'redbud'),\n",
       " (('critter', 'animal'), 'cod'),\n",
       " (('critter', 'animal'), 'queen'),\n",
       " (('critter', 'animal'), 'wet'),\n",
       " (('critter', 'animal'), 'greasewood'),\n",
       " (('half', 'part'), 'mallard'),\n",
       " (('half', 'part'), 'sadness'),\n",
       " (('half', 'part'), 'neural'),\n",
       " (('half', 'part'), 'join'),\n",
       " (('half', 'part'), 'idol'),\n",
       " (('key', 'metal'), 'enjoy'),\n",
       " (('key', 'metal'), 'lease'),\n",
       " (('key', 'metal'), 'rendezvous'),\n",
       " (('key', 'metal'), 'ethnicity'),\n",
       " (('key', 'metal'), 'ham'),\n",
       " (('key', 'pitch'), 'cock'),\n",
       " (('key', 'pitch'), 'honey'),\n",
       " (('key', 'pitch'), 'mallard'),\n",
       " (('key', 'pitch'), 'god'),\n",
       " (('key', 'pitch'), 'unsupervised'),\n",
       " (('key', 'device'), 'vermin'),\n",
       " (('key', 'device'), 'crystal'),\n",
       " (('key', 'device'), 'familial'),\n",
       " (('key', 'device'), 'strategy'),\n",
       " (('key', 'device'), 'rivalry'),\n",
       " (('holly', 'tree'), 'finch'),\n",
       " (('holly', 'tree'), 'affection'),\n",
       " (('holly', 'tree'), 'stone'),\n",
       " (('holly', 'tree'), 'ginger'),\n",
       " (('holly', 'tree'), 'rag'),\n",
       " (('holly', 'plant'), 'coati'),\n",
       " (('holly', 'plant'), 'pagan'),\n",
       " (('holly', 'plant'), 'reward'),\n",
       " (('holly', 'plant'), 'sheet'),\n",
       " (('holly', 'plant'), 'clarinet'),\n",
       " (('modal', 'structural'), 'half'),\n",
       " (('modal', 'structural'), 'turtle'),\n",
       " (('modal', 'structural'), 'lower'),\n",
       " (('modal', 'structural'), 'guava'),\n",
       " (('modal', 'structural'), 'horizon'),\n",
       " (('pig', 'ungulate'), 'echinoderm'),\n",
       " (('pig', 'ungulate'), 'screen'),\n",
       " (('pig', 'ungulate'), 'condition'),\n",
       " (('pig', 'ungulate'), 'familiar'),\n",
       " (('pig', 'ungulate'), 'surprise'),\n",
       " (('pig', 'creature'), 'star'),\n",
       " (('pig', 'creature'), 'honey'),\n",
       " (('pig', 'creature'), 'supply'),\n",
       " (('pig', 'creature'), 'beam'),\n",
       " (('pig', 'creature'), 'stoat'),\n",
       " (('pig', 'chordate'), 'cavity'),\n",
       " (('pig', 'chordate'), 'motorboat'),\n",
       " (('pig', 'chordate'), 'mandrake'),\n",
       " (('pig', 'chordate'), 'include'),\n",
       " (('pig', 'chordate'), 'refinery'),\n",
       " (('pig', 'beast'), 'couch'),\n",
       " (('pig', 'beast'), 'pay'),\n",
       " (('pig', 'beast'), 'population'),\n",
       " (('pig', 'beast'), 'school'),\n",
       " (('pig', 'beast'), 'banjo'),\n",
       " (('pig', 'swine'), 'cod'),\n",
       " (('pig', 'swine'), 'chill'),\n",
       " (('pig', 'swine'), 'prawn'),\n",
       " (('pig', 'swine'), 'hibiscus'),\n",
       " (('pig', 'swine'), 'salad'),\n",
       " (('pig', 'animal'), 'old'),\n",
       " (('pig', 'animal'), 'color'),\n",
       " (('pig', 'animal'), 'death'),\n",
       " (('pig', 'animal'), 'perch'),\n",
       " (('pig', 'animal'), 'rake'),\n",
       " (('pig', 'vertebrate'), 'accumulate'),\n",
       " (('pig', 'vertebrate'), 'stimulate'),\n",
       " (('pig', 'vertebrate'), 'basketball'),\n",
       " (('pig', 'vertebrate'), 'confuse'),\n",
       " (('pig', 'vertebrate'), 'unsettled'),\n",
       " (('pig', 'mammal'), 'missile'),\n",
       " (('pig', 'mammal'), 'humble'),\n",
       " (('pig', 'mammal'), 'pictorial'),\n",
       " (('pig', 'mammal'), 'compass'),\n",
       " (('pig', 'mammal'), 'seed'),\n",
       " (('potato', 'vine'), 'injure'),\n",
       " (('potato', 'vine'), 'clear'),\n",
       " (('potato', 'vine'), 'earth'),\n",
       " (('potato', 'vine'), 'hallway'),\n",
       " (('potato', 'vine'), 'ruminant'),\n",
       " (('potato', 'plant'), 'wall'),\n",
       " (('potato', 'plant'), 'seafood'),\n",
       " (('potato', 'plant'), 'copy'),\n",
       " (('potato', 'plant'), 'impose'),\n",
       " (('potato', 'plant'), 'mail'),\n",
       " (('potato', 'produce'), 'spontaneous'),\n",
       " (('potato', 'produce'), 'exchange'),\n",
       " (('potato', 'produce'), 'hope'),\n",
       " (('potato', 'produce'), 'dog'),\n",
       " (('potato', 'produce'), 'cowpea'),\n",
       " (('potato', 'vegetable'), 'step'),\n",
       " (('potato', 'vegetable'), 'dragonfly'),\n",
       " (('potato', 'vegetable'), 'library'),\n",
       " (('potato', 'vegetable'), 'chair'),\n",
       " (('potato', 'vegetable'), 'angel'),\n",
       " (('potato', 'tuber'), 'nematode'),\n",
       " (('potato', 'tuber'), 'cavity'),\n",
       " (('potato', 'tuber'), 'loon'),\n",
       " (('potato', 'tuber'), 'scavenger'),\n",
       " (('potato', 'tuber'), 'marry'),\n",
       " (('potato', 'food'), 'driver'),\n",
       " (('potato', 'food'), 'beans'),\n",
       " (('potato', 'food'), 'whale'),\n",
       " (('potato', 'food'), 'ranch'),\n",
       " (('potato', 'food'), 'brave'),\n",
       " (('potato', 'veggie'), 'commodity'),\n",
       " (('potato', 'veggie'), 'murder'),\n",
       " (('potato', 'veggie'), 'annual'),\n",
       " (('potato', 'veggie'), 'instrument'),\n",
       " (('potato', 'veggie'), 'decorate'),\n",
       " (('alleviation', 'injury'), 'spotty'),\n",
       " (('alleviation', 'injury'), 'inferior'),\n",
       " (('alleviation', 'injury'), 'myrtle'),\n",
       " (('alleviation', 'injury'), 'pupil'),\n",
       " (('alleviation', 'injury'), 'spade'),\n",
       " (('alleviation', 'medication'), 'ewe'),\n",
       " (('alleviation', 'medication'), 'orangutan'),\n",
       " (('alleviation', 'medication'), 'miss'),\n",
       " (('alleviation', 'medication'), 'bazooka'),\n",
       " (('alleviation', 'medication'), 'sorrow'),\n",
       " (('alleviation', 'pain'), 'sax'),\n",
       " (('alleviation', 'pain'), 'screen'),\n",
       " (('alleviation', 'pain'), 'snowmobile'),\n",
       " (('alleviation', 'pain'), 'locust'),\n",
       " (('alleviation', 'pain'), 'science'),\n",
       " (('alleviation', 'comfort'), 'effect'),\n",
       " (('alleviation', 'comfort'), 'cello'),\n",
       " (('alleviation', 'comfort'), 'lawn'),\n",
       " (('alleviation', 'comfort'), 'rattlesnake'),\n",
       " (('alleviation', 'comfort'), 'soldier'),\n",
       " (('alleviation', 'relief'), 'salamander'),\n",
       " (('alleviation', 'relief'), 'livestock'),\n",
       " (('alleviation', 'relief'), 'intoxicate'),\n",
       " (('alleviation', 'relief'), 'abuse'),\n",
       " (('alleviation', 'relief'), 'creature'),\n",
       " (('alleviation', 'eradication'), 'log'),\n",
       " (('alleviation', 'eradication'), 'clam'),\n",
       " (('alleviation', 'eradication'), 'breakfast'),\n",
       " (('alleviation', 'eradication'), 'curve'),\n",
       " (('alleviation', 'eradication'), 'plantain'),\n",
       " (('alleviation', 'decrease'), 'test'),\n",
       " (('alleviation', 'decrease'), 'art'),\n",
       " (('alleviation', 'decrease'), 'lawn'),\n",
       " (('alleviation', 'decrease'), 'journey'),\n",
       " (('alleviation', 'decrease'), 'innovator'),\n",
       " (('high', 'degree'), 'town'),\n",
       " (('high', 'degree'), 'evidence'),\n",
       " (('high', 'degree'), 'overflow'),\n",
       " (('high', 'degree'), 'socks'),\n",
       " (('high', 'degree'), 'tabulate'),\n",
       " (('divorce', 'separate'), 'defense'),\n",
       " (('divorce', 'separate'), 'wisteria'),\n",
       " (('divorce', 'separate'), 'lover'),\n",
       " (('divorce', 'separate'), 'beam'),\n",
       " (('divorce', 'separate'), 'skink'),\n",
       " (('divorce', 'separation'), 'low'),\n",
       " (('divorce', 'separation'), 'centaury'),\n",
       " (('divorce', 'separation'), 'pumpkin'),\n",
       " (('divorce', 'separation'), 'sunflower'),\n",
       " (('divorce', 'separation'), 'messy'),\n",
       " (('heat', 'change'), 'black'),\n",
       " (('heat', 'change'), 'conch'),\n",
       " (('heat', 'change'), 'pig'),\n",
       " (('heat', 'change'), 'root'),\n",
       " (('heat', 'change'), 'mountain'),\n",
       " (('heat', 'race'), 'dense'),\n",
       " (('heat', 'race'), 'apron'),\n",
       " (('heat', 'race'), 'sundew'),\n",
       " (('heat', 'race'), 'spring'),\n",
       " (('heat', 'race'), 'brick'),\n",
       " (('heat', 'energy'), 'text'),\n",
       " (('heat', 'energy'), 'internal'),\n",
       " (('heat', 'energy'), 'dill'),\n",
       " (('heat', 'energy'), 'orbital'),\n",
       " (('heat', 'energy'), 'hen'),\n",
       " (('cannon', 'implement'), 'impala'),\n",
       " (('cannon', 'implement'), 'coral'),\n",
       " (('cannon', 'implement'), 'bittersweet'),\n",
       " (('cannon', 'implement'), 'python'),\n",
       " (('cannon', 'implement'), 'tabulate'),\n",
       " (('cannon', 'device'), 'drug'),\n",
       " (('cannon', 'device'), 'wax'),\n",
       " (('cannon', 'device'), 'grape'),\n",
       " (('cannon', 'device'), 'base'),\n",
       " (('cannon', 'device'), 'lettuce'),\n",
       " (('cannon', 'object'), 'element'),\n",
       " (('cannon', 'object'), 'lorry'),\n",
       " (('cannon', 'object'), 'dinner'),\n",
       " (('cannon', 'object'), 'parrot'),\n",
       " (('cannon', 'object'), 'cup'),\n",
       " (('cannon', 'artifact'), 'knowledge'),\n",
       " (('cannon', 'artifact'), 'rice'),\n",
       " (('cannon', 'artifact'), 'bronco'),\n",
       " (('cannon', 'artifact'), 'charm'),\n",
       " (('cannon', 'artifact'), 'sea'),\n",
       " (('cannon', 'artefact'), 'chimpanzee'),\n",
       " (('cannon', 'artefact'), 'tapir'),\n",
       " (('cannon', 'artefact'), 'fluid'),\n",
       " (('cannon', 'artefact'), 'camper'),\n",
       " (('cannon', 'artefact'), 'murder'),\n",
       " (('cannon', 'weapon'), 'hackberry'),\n",
       " (('cannon', 'weapon'), 'boil'),\n",
       " (('cannon', 'weapon'), 'arrowroot'),\n",
       " (('cannon', 'weapon'), 'medicine'),\n",
       " (('cannon', 'weapon'), 'pleasure'),\n",
       " (('cannon', 'arm'), 'locomotive'),\n",
       " (('cannon', 'arm'), 'casuarina'),\n",
       " (('cannon', 'arm'), 'trim'),\n",
       " (('cannon', 'arm'), 'pride'),\n",
       " (('cannon', 'arm'), 'cricket'),\n",
       " (('cannon', 'gun'), 'hope'),\n",
       " (('cannon', 'gun'), 'college'),\n",
       " (('cannon', 'gun'), 'joint'),\n",
       " (('cannon', 'gun'), 'test'),\n",
       " (('cannon', 'gun'), 'shore'),\n",
       " (('boy', 'man'), 'tuna'),\n",
       " (('boy', 'man'), 'sit'),\n",
       " (('boy', 'man'), 'employment'),\n",
       " (('boy', 'man'), 'woman'),\n",
       " (('boy', 'man'), 'invalid'),\n",
       " (('boy', 'animal'), 'library'),\n",
       " (('boy', 'animal'), 'known'),\n",
       " (('boy', 'animal'), 'workforce'),\n",
       " (('boy', 'animal'), 'pitch'),\n",
       " (('boy', 'animal'), 'jar'),\n",
       " (('boy', 'child'), 'insect'),\n",
       " (('boy', 'child'), 'lady'),\n",
       " (('boy', 'child'), 'battle'),\n",
       " (('boy', 'child'), 'viper'),\n",
       " (('boy', 'child'), 'cry'),\n",
       " (('barberry', 'plant'), 'hydrangea'),\n",
       " (('barberry', 'plant'), 'tsetse'),\n",
       " (('barberry', 'plant'), 'stingray'),\n",
       " (('barberry', 'plant'), 'population'),\n",
       " (('barberry', 'plant'), 'original'),\n",
       " (('barberry', 'shrub'), 'knitwear'),\n",
       " (('barberry', 'shrub'), 'equip'),\n",
       " (('barberry', 'shrub'), 'gooseberry'),\n",
       " (('barberry', 'shrub'), 'fun'),\n",
       " (('barberry', 'shrub'), 'groundhog'),\n",
       " (('creation', 'begin'), 'feeling'),\n",
       " (('creation', 'begin'), 'screwdriver'),\n",
       " (('creation', 'begin'), 'saw'),\n",
       " (('creation', 'begin'), 'aphid'),\n",
       " (('creation', 'begin'), 'pleasure'),\n",
       " (('musician', 'performer'), 'grebe'),\n",
       " (('musician', 'performer'), 'hazel'),\n",
       " (('musician', 'performer'), 'pleasure'),\n",
       " (('musician', 'performer'), 'bandicoot'),\n",
       " (('musician', 'performer'), 'extinguish'),\n",
       " (('musician', 'vocation'), 'dugong'),\n",
       " (('musician', 'vocation'), 'broom'),\n",
       " (('musician', 'vocation'), 'quilt'),\n",
       " (('musician', 'vocation'), 'sparkle'),\n",
       " (('musician', 'vocation'), 'earthly'),\n",
       " (('musician', 'occupation'), 'forgive'),\n",
       " (('musician', 'occupation'), 'bug'),\n",
       " (('musician', 'occupation'), 'phrase'),\n",
       " (('musician', 'occupation'), 'support'),\n",
       " (('musician', 'occupation'), 'soprano'),\n",
       " (('musician', 'career'), 'narrow'),\n",
       " (('musician', 'career'), 'split'),\n",
       " (('musician', 'career'), 'highway'),\n",
       " (('musician', 'career'), 'joint'),\n",
       " (('musician', 'career'), 'carnivore'),\n",
       " (('scavenger', 'animal'), 'staff'),\n",
       " (('scavenger', 'animal'), 'account'),\n",
       " (('scavenger', 'animal'), 'simplify'),\n",
       " (('scavenger', 'animal'), 'hide'),\n",
       " (('scavenger', 'animal'), 'determine'),\n",
       " (('copepod', 'arthropod'), 'i'),\n",
       " (('copepod', 'arthropod'), 'pay'),\n",
       " (('copepod', 'arthropod'), 'sundew'),\n",
       " (('copepod', 'arthropod'), 'phytoplankton'),\n",
       " (('copepod', 'arthropod'), 'physics'),\n",
       " (('copepod', 'invertebrate'), 'eradication'),\n",
       " (('copepod', 'invertebrate'), 'clay'),\n",
       " (('copepod', 'invertebrate'), 'inhabit'),\n",
       " (('copepod', 'invertebrate'), 'treat'),\n",
       " (('copepod', 'invertebrate'), 'exchange'),\n",
       " (('copepod', 'animal'), 'temperature'),\n",
       " (('copepod', 'animal'), 'dance'),\n",
       " (('copepod', 'animal'), 'hope'),\n",
       " (('copepod', 'animal'), 'wolf'),\n",
       " (('copepod', 'animal'), 'purpose'),\n",
       " (('copepod', 'crustacean'), 'slug'),\n",
       " (('copepod', 'crustacean'), 'optical'),\n",
       " (('copepod', 'crustacean'), 'hawthorn'),\n",
       " (('copepod', 'crustacean'), 'teak'),\n",
       " (('copepod', 'crustacean'), 'teff'),\n",
       " (('lobelia', 'plant'), 'turtle'),\n",
       " (('lobelia', 'plant'), 'beetroot'),\n",
       " (('lobelia', 'plant'), 'board'),\n",
       " (('lobelia', 'plant'), 'mussel'),\n",
       " (('lobelia', 'plant'), 'independence'),\n",
       " (('lobelia', 'herb'), 'infect'),\n",
       " (('lobelia', 'herb'), 'funded'),\n",
       " (('lobelia', 'herb'), 'repeat'),\n",
       " (('lobelia', 'herb'), 'fix'),\n",
       " (('lobelia', 'herb'), 'metamorphosis'),\n",
       " (('aster', 'flower'), 'hazelnut'),\n",
       " (('aster', 'flower'), 'steamboat'),\n",
       " (('aster', 'flower'), 'mammoth'),\n",
       " (('aster', 'flower'), 'whale'),\n",
       " (('aster', 'flower'), 'salad'),\n",
       " (('aster', 'plant'), 'hold'),\n",
       " (('aster', 'plant'), 'green'),\n",
       " (('aster', 'plant'), 'bias'),\n",
       " (('aster', 'plant'), 'pollinator'),\n",
       " (('aster', 'plant'), 'leak'),\n",
       " (('aster', 'angiosperm'), 'more'),\n",
       " (('aster', 'angiosperm'), 'meeting'),\n",
       " (('aster', 'angiosperm'), 'orange_juice'),\n",
       " (('aster', 'angiosperm'), 'cello'),\n",
       " (('aster', 'angiosperm'), 'bake'),\n",
       " (('thistle', 'weed'), 'crowd'),\n",
       " (('thistle', 'weed'), 'material'),\n",
       " (('thistle', 'weed'), 'valerian'),\n",
       " (('thistle', 'weed'), 'steal'),\n",
       " (('thistle', 'weed'), 'springbok'),\n",
       " (('thistle', 'plant'), 'agapanthus'),\n",
       " (('thistle', 'plant'), 'normative'),\n",
       " (('thistle', 'plant'), 'communicate'),\n",
       " (('thistle', 'plant'), 'arrange'),\n",
       " (('thistle', 'plant'), 'nervous'),\n",
       " (('cat', 'chordate'), 'hood'),\n",
       " (('cat', 'chordate'), 'tool'),\n",
       " (('cat', 'chordate'), 'consume'),\n",
       " (('cat', 'chordate'), 'lift'),\n",
       " (('cat', 'chordate'), 'regular'),\n",
       " (('cat', 'carnivore'), 'camper'),\n",
       " (('cat', 'carnivore'), 'unsafe'),\n",
       " (('cat', 'carnivore'), 'mitten'),\n",
       " (('cat', 'carnivore'), 'globe'),\n",
       " (('cat', 'carnivore'), 'belief'),\n",
       " (('cat', 'plant'), 'smoke'),\n",
       " (('cat', 'plant'), 'boil'),\n",
       " (('cat', 'plant'), 'standardized'),\n",
       " (('cat', 'plant'), 'pierce'),\n",
       " (('cat', 'plant'), 'study'),\n",
       " (('cat', 'placental'), 'lace'),\n",
       " (('cat', 'placental'), 'watch'),\n",
       " (('cat', 'placental'), 'foxglove'),\n",
       " (('cat', 'placental'), 'post'),\n",
       " (('cat', 'placental'), 'transport'),\n",
       " (('cat', 'feline'), 'detail'),\n",
       " (('cat', 'feline'), 'burro'),\n",
       " (('cat', 'feline'), 'hop'),\n",
       " (('cat', 'feline'), 'tavern'),\n",
       " (('cat', 'feline'), 'ragwort'),\n",
       " (('cat', 'cattail'), 'uneven'),\n",
       " (('cat', 'cattail'), 'grass'),\n",
       " (('cat', 'cattail'), 'brain'),\n",
       " (('cat', 'cattail'), 'rag'),\n",
       " (('cat', 'cattail'), 'bramble'),\n",
       " (('cat', 'vertebrate'), 'consume'),\n",
       " (('cat', 'vertebrate'), 'flea'),\n",
       " (('cat', 'vertebrate'), 'flyer'),\n",
       " (('cat', 'vertebrate'), 'hovercraft'),\n",
       " (('cat', 'vertebrate'), 'soursop'),\n",
       " (('cat', 'mammal'), 'camper'),\n",
       " (('cat', 'mammal'), 'cloth'),\n",
       " (('cat', 'mammal'), 'kitchenware'),\n",
       " (('cat', 'mammal'), 'hen'),\n",
       " (('cat', 'mammal'), 'probable'),\n",
       " (('cat', 'animal'), 'earthly'),\n",
       " (('cat', 'animal'), 'wash'),\n",
       " (('cat', 'animal'), 'bazooka'),\n",
       " (('cat', 'animal'), 'vital'),\n",
       " (('cat', 'animal'), 'wax'),\n",
       " (('cat', 'beast'), 'scalpel'),\n",
       " (('cat', 'beast'), 'floor'),\n",
       " (('cat', 'beast'), 'cap'),\n",
       " (('cat', 'beast'), 'conclude'),\n",
       " (('cat', 'beast'), 'shower'),\n",
       " (('cat', 'pet'), 'coin'),\n",
       " (('cat', 'pet'), 'climb'),\n",
       " (('cat', 'pet'), 'artifact'),\n",
       " (('cat', 'pet'), 'damage'),\n",
       " (('cat', 'pet'), 'cello'),\n",
       " (('cat', 'creature'), 'cry'),\n",
       " (('cat', 'creature'), 'eucalyptus'),\n",
       " (('cat', 'creature'), 'cycad'),\n",
       " (('cat', 'creature'), 'set'),\n",
       " (('cat', 'creature'), 'great'),\n",
       " (('cat', 'friend'), 'speaker'),\n",
       " (('cat', 'friend'), 'man'),\n",
       " (('cat', 'friend'), 'lugworm'),\n",
       " (('cat', 'friend'), 'yak'),\n",
       " (('cat', 'friend'), 'passerine'),\n",
       " (('bath', 'place'), 'chill'),\n",
       " (('bath', 'place'), 'quench'),\n",
       " (('bath', 'place'), 'pumpkin'),\n",
       " (('bath', 'place'), 'rider'),\n",
       " (('bath', 'place'), 'steak'),\n",
       " (('price', 'value'), 'jasmine'),\n",
       " (('price', 'value'), 'outside'),\n",
       " (('price', 'value'), 'cougar'),\n",
       " (('price', 'value'), 'caiman'),\n",
       " (('price', 'value'), 'land'),\n",
       " (('watermelon', 'melon'), 'press'),\n",
       " (('watermelon', 'melon'), 'missile'),\n",
       " (('watermelon', 'melon'), 'take'),\n",
       " (('watermelon', 'melon'), 'store'),\n",
       " (('watermelon', 'melon'), 'thoroughbred'),\n",
       " (('watermelon', 'vine'), 'unbroken'),\n",
       " (('watermelon', 'vine'), 'park'),\n",
       " (('watermelon', 'vine'), 'fat'),\n",
       " (('watermelon', 'vine'), 'anger'),\n",
       " (('watermelon', 'vine'), 'polish'),\n",
       " (('watermelon', 'gourd'), 'liana'),\n",
       " (('watermelon', 'gourd'), 'crook'),\n",
       " (('watermelon', 'gourd'), 'disk'),\n",
       " (('watermelon', 'gourd'), 'range'),\n",
       " (('watermelon', 'gourd'), 'weasel'),\n",
       " (('watermelon', 'plant'), 'pansy'),\n",
       " (('watermelon', 'plant'), 'history'),\n",
       " (('watermelon', 'plant'), 'neem'),\n",
       " (('watermelon', 'plant'), 'escape'),\n",
       " (('watermelon', 'plant'), 'chard'),\n",
       " (('laugh', 'fun'), 'magpie'),\n",
       " (('laugh', 'fun'), 'clinic'),\n",
       " (('laugh', 'fun'), 'blouse'),\n",
       " (('laugh', 'fun'), 'rubber'),\n",
       " (('laugh', 'fun'), 'defeat'),\n",
       " (('worm', 'invertebrate'), 'snapdragon'),\n",
       " (('worm', 'invertebrate'), 'grammatical'),\n",
       " (('worm', 'invertebrate'), 'string'),\n",
       " (('worm', 'invertebrate'), 'happen'),\n",
       " (('worm', 'invertebrate'), 'indispensable'),\n",
       " (('worm', 'animal'), 'workforce'),\n",
       " (('worm', 'animal'), 'canine'),\n",
       " (('worm', 'animal'), 'honesty'),\n",
       " (('worm', 'animal'), 'code'),\n",
       " (('worm', 'animal'), 'skin'),\n",
       " (('spade', 'object'), 'thing'),\n",
       " (('spade', 'object'), 'mitten'),\n",
       " (('spade', 'object'), 'slam'),\n",
       " (('spade', 'object'), 'union'),\n",
       " (('spade', 'object'), 'vessel'),\n",
       " (('spade', 'tool'), 'robot'),\n",
       " (('spade', 'tool'), 'fungus'),\n",
       " (('spade', 'tool'), 'design'),\n",
       " (('spade', 'tool'), 'illegal'),\n",
       " (('spade', 'tool'), 'low'),\n",
       " (('spade', 'utensil'), 'pond'),\n",
       " (('spade', 'utensil'), 'deep'),\n",
       " (('spade', 'utensil'), 'adore'),\n",
       " (('spade', 'utensil'), 'progress'),\n",
       " (('spade', 'utensil'), 'capybara'),\n",
       " (('spade', 'artefact'), 'converse'),\n",
       " (('spade', 'artefact'), 'hen'),\n",
       " (('spade', 'artefact'), 'train'),\n",
       " (('spade', 'artefact'), 'sight'),\n",
       " (('spade', 'artefact'), 'habitation'),\n",
       " (('spade', 'implement'), 'immortal'),\n",
       " (('spade', 'implement'), 'pike'),\n",
       " (('spade', 'implement'), 'point'),\n",
       " (('spade', 'implement'), 'wall'),\n",
       " (('spade', 'implement'), 'boy'),\n",
       " (('spade', 'artifact'), 'fish'),\n",
       " (('spade', 'artifact'), 'catmint'),\n",
       " (('spade', 'artifact'), 'vessel'),\n",
       " (('spade', 'artifact'), 'goldenrod'),\n",
       " (('spade', 'artifact'), 'make'),\n",
       " (('fitness', 'magazine'), 'boa'),\n",
       " (('fitness', 'magazine'), 'flex'),\n",
       " (('fitness', 'magazine'), 'stain'),\n",
       " (('fitness', 'magazine'), 'achillea'),\n",
       " (('fitness', 'magazine'), 'turtle'),\n",
       " (('jar', 'drinkware'), 'me'),\n",
       " (('jar', 'drinkware'), 'unclear'),\n",
       " (('jar', 'drinkware'), 'pastor'),\n",
       " (('jar', 'drinkware'), 'cycad'),\n",
       " (('jar', 'drinkware'), 'stimulate'),\n",
       " (('jar', 'utensil'), 'touch'),\n",
       " (('jar', 'utensil'), 'instruction'),\n",
       " (('jar', 'utensil'), 'agree'),\n",
       " (('jar', 'utensil'), 'craft'),\n",
       " (('jar', 'utensil'), 'alewife'),\n",
       " (('jar', 'kitchenware'), 'mandarin'),\n",
       " (('jar', 'kitchenware'), 'cowpea'),\n",
       " (('jar', 'kitchenware'), 'deny'),\n",
       " (('jar', 'kitchenware'), 'battleship'),\n",
       " (('jar', 'kitchenware'), 'pear'),\n",
       " (('jar', 'container'), 'gorgonian'),\n",
       " (('jar', 'container'), 'edinburgh'),\n",
       " (('jar', 'container'), 'gadget'),\n",
       " (('jar', 'container'), 'casuarina'),\n",
       " (('jar', 'container'), 'diagnose'),\n",
       " (('jar', 'object'), 'mango'),\n",
       " (('jar', 'object'), 'shade'),\n",
       " (('jar', 'object'), 'crush'),\n",
       " (('jar', 'object'), 'scooter'),\n",
       " (('jar', 'object'), 'forth'),\n",
       " (('jar', 'vessel'), 'pan'),\n",
       " (('jar', 'vessel'), 'illegal'),\n",
       " (('jar', 'vessel'), 'exit'),\n",
       " (('jar', 'vessel'), 'purpose'),\n",
       " (('jar', 'vessel'), 'toilet'),\n",
       " (('jar', 'artefact'), 'hole'),\n",
       " (('jar', 'artefact'), 'salmon'),\n",
       " (('jar', 'artefact'), 'strategy'),\n",
       " (('jar', 'artefact'), 'thick'),\n",
       " (('jar', 'artefact'), 'spend'),\n",
       " (('jar', 'artifact'), 'python'),\n",
       " (('jar', 'artifact'), 'measurement'),\n",
       " (('jar', 'artifact'), 'up'),\n",
       " (('jar', 'artifact'), 'cholla'),\n",
       " (('jar', 'artifact'), 'famous'),\n",
       " (('kid', 'mammal'), 'birthday'),\n",
       " (('kid', 'mammal'), 'travel'),\n",
       " (('kid', 'mammal'), 'state'),\n",
       " (('kid', 'mammal'), 'liana'),\n",
       " (('kid', 'mammal'), 'indigo'),\n",
       " (('kid', 'animal'), 'oats'),\n",
       " (('kid', 'animal'), 'buck'),\n",
       " (('kid', 'animal'), 'crush'),\n",
       " (('kid', 'animal'), 'abuse'),\n",
       " (('kid', 'animal'), 'sweet'),\n",
       " (('kid', 'chordate'), 'old'),\n",
       " (('kid', 'chordate'), 'paranormal'),\n",
       " (('kid', 'chordate'), 'stoat'),\n",
       " (('kid', 'chordate'), 'walnut'),\n",
       " (('kid', 'chordate'), 'water_vapor'),\n",
       " (('kid', 'goat'), 'ox'),\n",
       " (('kid', 'goat'), 'audition'),\n",
       " (('kid', 'goat'), 'forklift'),\n",
       " (('kid', 'goat'), 'tapeworm'),\n",
       " (('kid', 'goat'), 'immortal'),\n",
       " (('kid', 'placental'), 'concise'),\n",
       " (('kid', 'placental'), 'limpet'),\n",
       " (('kid', 'placental'), 'grape'),\n",
       " (('kid', 'placental'), 'peppermint'),\n",
       " (('kid', 'placental'), 'manta'),\n",
       " (('kid', 'ruminant'), 'statement'),\n",
       " (('kid', 'ruminant'), 'deal'),\n",
       " (('kid', 'ruminant'), 'escape'),\n",
       " (('kid', 'ruminant'), 'orbital'),\n",
       " (('kid', 'ruminant'), 'education'),\n",
       " (('kid', 'vertebrate'), 'plan'),\n",
       " (('kid', 'vertebrate'), 'music'),\n",
       " (('kid', 'vertebrate'), 'print'),\n",
       " (('kid', 'vertebrate'), 'cockroach'),\n",
       " (('kid', 'vertebrate'), 'confidence'),\n",
       " (('kid', 'ungulate'), 'humility'),\n",
       " (('kid', 'ungulate'), 'mail'),\n",
       " (('kid', 'ungulate'), 'construct'),\n",
       " (('kid', 'ungulate'), 'contrast'),\n",
       " (('kid', 'ungulate'), 'slam'),\n",
       " (('kid', 'person'), 'attitude'),\n",
       " (('kid', 'person'), 'accumulate'),\n",
       " (('kid', 'person'), 'brass'),\n",
       " (('kid', 'person'), 'suppose'),\n",
       " (('kid', 'person'), 'caterpillar'),\n",
       " (('kid', 'offspring'), 'jay'),\n",
       " (('kid', 'offspring'), 'thumb'),\n",
       " (('kid', 'offspring'), 'feeling'),\n",
       " (('kid', 'offspring'), 'word'),\n",
       " (('kid', 'offspring'), 'pan'),\n",
       " (('steam', 'clean'), 'prosecution'),\n",
       " (('steam', 'clean'), 'wildcat'),\n",
       " (('steam', 'clean'), 'saltbush'),\n",
       " (('steam', 'clean'), 'firearm'),\n",
       " (('steam', 'clean'), 'banksia'),\n",
       " (('steam', 'cook'), 'flower'),\n",
       " (('steam', 'cook'), 'red'),\n",
       " (('steam', 'cook'), 'unacceptable'),\n",
       " (('steam', 'cook'), 'insect'),\n",
       " (('steam', 'cook'), 'snowmobile'),\n",
       " (('steam', 'anger'), 'undying'),\n",
       " (('steam', 'anger'), 'clear'),\n",
       " (('steam', 'anger'), 'cow'),\n",
       " (('steam', 'anger'), 'grapefruit'),\n",
       " (('steam', 'anger'), 'suggest'),\n",
       " (('steam', 'water'), 'seaplane'),\n",
       " (('steam', 'water'), 'question'),\n",
       " (('steam', 'water'), 'cave'),\n",
       " (('steam', 'water'), 'aid'),\n",
       " (('steam', 'water'), 'rag'),\n",
       " (('lizard', 'chordate'), 'river'),\n",
       " (('lizard', 'chordate'), 'slug'),\n",
       " (('lizard', 'chordate'), 'reindeer'),\n",
       " (('lizard', 'chordate'), 'termite'),\n",
       " (('lizard', 'chordate'), 'nectarine'),\n",
       " (('lizard', 'vertebrate'), 'familiarize'),\n",
       " (('lizard', 'vertebrate'), 'adder'),\n",
       " (('lizard', 'vertebrate'), 'mandarin'),\n",
       " (('lizard', 'vertebrate'), 'it'),\n",
       " (('lizard', 'vertebrate'), 'hear'),\n",
       " (('lizard', 'saurian'), 'gas'),\n",
       " (('lizard', 'saurian'), 'crush'),\n",
       " (('lizard', 'saurian'), 'hope'),\n",
       " (('lizard', 'saurian'), 'waterfall'),\n",
       " (('lizard', 'saurian'), 'cancer'),\n",
       " (('lizard', 'animal'), 'iguana'),\n",
       " (('lizard', 'animal'), 'pineapple'),\n",
       " (('lizard', 'animal'), 'backpack'),\n",
       " (('lizard', 'animal'), 'stoat'),\n",
       " (('lizard', 'animal'), 'robot'),\n",
       " (('lizard', 'reptile'), 'tribe'),\n",
       " (('lizard', 'reptile'), 'hyena'),\n",
       " (('lizard', 'reptile'), 'column'),\n",
       " (('lizard', 'reptile'), 'farm'),\n",
       " (('lizard', 'reptile'), 'incline'),\n",
       " (('lizard', 'beast'), 'climatic'),\n",
       " (('lizard', 'beast'), 'temperature'),\n",
       " (('lizard', 'beast'), 'tumbler'),\n",
       " (('lizard', 'beast'), 'dracaena'),\n",
       " (('lizard', 'beast'), 'courage'),\n",
       " (('lizard', 'carnivore'), 'breathe'),\n",
       " (('lizard', 'carnivore'), 'barracuda'),\n",
       " (('lizard', 'carnivore'), 'mountain'),\n",
       " (('lizard', 'carnivore'), 'stretcher'),\n",
       " (('lizard', 'carnivore'), 'drill'),\n",
       " (('lizard', 'creature'), 'pachyderm'),\n",
       " (('lizard', 'creature'), 'hop'),\n",
       " (('lizard', 'creature'), 'skateboard'),\n",
       " (('lizard', 'creature'), 'document'),\n",
       " (('lizard', 'creature'), 'plant'),\n",
       " (('lace', 'cloth'), 'cottage'),\n",
       " (('lace', 'cloth'), 'red'),\n",
       " (('lace', 'cloth'), 'refuse'),\n",
       " (('lace', 'cloth'), 'sadness'),\n",
       " (('lace', 'cloth'), 'romantic'),\n",
       " (('lace', 'fabric'), 'fur'),\n",
       " (('lace', 'fabric'), 'fever'),\n",
       " (('lace', 'fabric'), 'cutlery'),\n",
       " (('lace', 'fabric'), 'bumpy'),\n",
       " (('lace', 'fabric'), 'township'),\n",
       " (('lace', 'tie'), 'candy'),\n",
       " (('lace', 'tie'), 'creativity'),\n",
       " (('lace', 'tie'), 'aloe'),\n",
       " (('lace', 'tie'), 'bone'),\n",
       " (('lace', 'tie'), 'gladiolus'),\n",
       " (('kangaroo', 'animal'), 'songbird'),\n",
       " (('kangaroo', 'animal'), 'associative'),\n",
       " (('kangaroo', 'animal'), 'shallow'),\n",
       " (('kangaroo', 'animal'), 'bloodworm'),\n",
       " (('kangaroo', 'animal'), 'chest'),\n",
       " (('kangaroo', 'marsupial'), 'old'),\n",
       " (('kangaroo', 'marsupial'), 'carpet'),\n",
       " (('kangaroo', 'marsupial'), 'cypress'),\n",
       " (('kangaroo', 'marsupial'), 'goal'),\n",
       " (('kangaroo', 'marsupial'), 'date'),\n",
       " (('kangaroo', 'mammal'), 'vague'),\n",
       " (('kangaroo', 'mammal'), 'tread'),\n",
       " (('kangaroo', 'mammal'), 'cactus'),\n",
       " (('kangaroo', 'mammal'), 'pouch'),\n",
       " (('kangaroo', 'mammal'), 'airship'),\n",
       " (('kangaroo', 'vertebrate'), 'physical'),\n",
       " (('kangaroo', 'vertebrate'), 'manufacture'),\n",
       " (('kangaroo', 'vertebrate'), 'concentrate'),\n",
       " (('kangaroo', 'vertebrate'), 'illegal'),\n",
       " (('kangaroo', 'vertebrate'), 'ham'),\n",
       " (('kangaroo', 'chordate'), 'succeed'),\n",
       " (('kangaroo', 'chordate'), 'fig'),\n",
       " (('kangaroo', 'chordate'), 'finger'),\n",
       " (('kangaroo', 'chordate'), 'writer'),\n",
       " (('kangaroo', 'chordate'), 'toilet'),\n",
       " (('confidence', 'emotion'), 'endemic'),\n",
       " (('confidence', 'emotion'), 'damage'),\n",
       " (('confidence', 'emotion'), 'stand'),\n",
       " (('confidence', 'emotion'), 'barrow'),\n",
       " (('confidence', 'emotion'), 'liberal'),\n",
       " (('despotic', 'undemocratic'), 'pear'),\n",
       " (('despotic', 'undemocratic'), 'bloodroot'),\n",
       " (('despotic', 'undemocratic'), 'converse'),\n",
       " (('despotic', 'undemocratic'), 'invalidate'),\n",
       " (('despotic', 'undemocratic'), 'illegal'),\n",
       " (('despotic', 'commanding'), 'spoil'),\n",
       " (('despotic', 'commanding'), 'log'),\n",
       " (('despotic', 'commanding'), 'tricycle'),\n",
       " (('despotic', 'commanding'), 'wallet'),\n",
       " (('despotic', 'commanding'), 'humility'),\n",
       " (('despotic', 'mean'), 'grapefruit'),\n",
       " (('despotic', 'mean'), 'age'),\n",
       " (('despotic', 'mean'), 'personnel'),\n",
       " (('despotic', 'mean'), 'galax'),\n",
       " (('despotic', 'mean'), 'lot'),\n",
       " (('despotic', 'cruel'), 'geek'),\n",
       " (('despotic', 'cruel'), 'snowmobile'),\n",
       " (('despotic', 'cruel'), 'coati'),\n",
       " (('despotic', 'cruel'), 'iconic'),\n",
       " (('despotic', 'cruel'), 'ceiling'),\n",
       " (('despotic', 'absolute'), 'disagreement'),\n",
       " (('despotic', 'absolute'), 'refinery'),\n",
       " (('despotic', 'absolute'), 'defeat'),\n",
       " (('despotic', 'absolute'), 'cast'),\n",
       " (('despotic', 'absolute'), 'restaurant'),\n",
       " (('lady', 'woman'), 'raise'),\n",
       " (('lady', 'woman'), 'aisle'),\n",
       " (('lady', 'woman'), 'tugboat'),\n",
       " (('lady', 'woman'), 'collection'),\n",
       " (('lady', 'woman'), 'cabinet'),\n",
       " (('coconut', 'plant'), 'coneflower'),\n",
       " (('coconut', 'plant'), 'anvil'),\n",
       " (('coconut', 'plant'), 'revolving'),\n",
       " (('coconut', 'plant'), 'sunset'),\n",
       " (('coconut', 'plant'), 'credit'),\n",
       " ...]"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_negative_tuples((data.test_query, data.test_hyper), data, get_random_negative_words, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function that returns negative samples alongside set of positive samples\n",
    "# we need to pass:\n",
    "# the batch hyponym terms, batch of hypernym terms, negative_tuples, tokenizer \n",
    "# to create sequences\n",
    "def extend_batch_with_negatives(batch_X_term, batch_X_hyper, negative_tuples,                              \n",
    "                                tokenizer):\n",
    "    # initialise negative tuples container\n",
    "    positive_words = [(tokenizer.index_word[term_id], tokenizer.index_word[hyper_id]) \\\n",
    "                          for term_id, hyper_id in zip(batch_X_term.flatten(), batch_X_hyper.flatten())]\n",
    "    \n",
    "    # tokenize -ve samples\n",
    "    neg_terms, neg_hyper = zip(*[(qh[0], h) for qh, h in negative_tuples if qh in positive_words])\n",
    "    \n",
    "    neg_terms_seq = tokenizer.texts_to_sequences(neg_terms)\n",
    "    neg_hyper_seq = tokenizer.texts_to_sequences(neg_hyper)\n",
    "\n",
    "    # before increasing size of our batch, let's set the actual y values\n",
    "    # the first n terms are true (1s), and the rest are the -ve samples (0)\n",
    "    batch_y_label = np.concatenate((\n",
    "            np.ones(batch_X_term.shape[0]),\n",
    "            np.zeros(len(neg_terms_seq))\n",
    "    ))\n",
    "    # finally, stack -ve sequences at the bottom of +ves to \n",
    "    # create our final training batch\n",
    "    # at most, batch size will be 192 samples            \n",
    "\n",
    "    batch_X_term = np.vstack((batch_X_term, np.array(neg_terms_seq)))\n",
    "    batch_X_hyper = np.vstack((batch_X_hyper, np.array(neg_hyper_seq)))\n",
    "    \n",
    "    return batch_X_term, batch_X_hyper, batch_y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Reshape, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import UnitNorm\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "# Phi layer initialiser\n",
    "def random_identity(shape, dtype=\"float32\", partition_info=None):    \n",
    "    rnorm = K.random_normal((shape[-1],shape[-1]), mean=0., stddev=0.05)\n",
    "    #identity = K.eye(shape[-1], dtype='float32')        \n",
    "    rident = tf.eye(shape[-1]) * rnorm\n",
    "    return rident\n",
    "\n",
    "def random_normal(shape, dtype=\"float32\", partition_info=None): \n",
    "    return K.random_normal((shape[-1],shape[-1]), \n",
    "                             mean=0., stddev=0.05) \n",
    "\n",
    "def get_CRIM_model(phi_k=1, train_embeddings=False,\\\n",
    "                   embeddings_dim=300, vocab_size=1000,\\\n",
    "                   embeddings_matrix=None,\n",
    "                   phi_init = None,\n",
    "                   phi_activity_regularisation = None,\n",
    "                   sigmoid_kernel_regularisation = None,\n",
    "                   sigmoid_bias_regularisation = None,\n",
    "                   sigmoid_kernel_constraint = None,\n",
    "                   do_dropout = False\n",
    "                  ):\n",
    "    \n",
    "    hypo_input  = Input(shape=(1,), name='Hyponym')\n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    embedding_layer = Embedding(vocab_size + 1, embeddings_dim, embeddings_constraint = UnitNorm(axis=1), \n",
    "                                name='TermEmbedding')\n",
    "    \n",
    "    \n",
    "    hypo_embedding = embedding_layer(hypo_input)    \n",
    "    hyper_embedding = embedding_layer(hyper_input)\n",
    "    \n",
    "    # Add Dropout to avoid overfit\n",
    "    if do_dropout:\n",
    "        hypo_embedding = Dropout(0.25)(hypo_embedding)\n",
    "        hyper_embedding = Dropout(0.25)(hyper_embedding)\n",
    "    \n",
    "    phi_layer = []\n",
    "    for i in range(phi_k):\n",
    "        phi_layer.append(Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                               activity_regularizer=phi_activity_regularisation,\n",
    "                               kernel_initializer=phi_init,                               \n",
    "                               name='Phi%d' % (i))(hypo_embedding))\n",
    "\n",
    "    #phi1 = Dense(embeddings_dim, activation=None, use_bias=False, \n",
    "                #kernel_initializer=random_identity, name='Phi1')(hypo_embedding)\n",
    "\n",
    "    if phi_k == 1:\n",
    "        # flatten tensors\n",
    "        phi = Flatten()(phi_layer[0])\n",
    "        hyper_embedding = Flatten()(hyper_embedding)    \n",
    "    else:\n",
    "        phi = concatenate(phi_layer, axis=1)\n",
    "\n",
    "    \n",
    "    # this is referred to as \"s\" in the \"CRIM\" paper    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])\n",
    "    \n",
    "    if phi_k > 1:\n",
    "        phi_hyper = Flatten()(phi_hyper)\n",
    "    \n",
    "    predictions = Dense(1, activation=\"sigmoid\", name='Prediction',\n",
    "                        use_bias=True,\n",
    "                        kernel_initializer=Ones,\n",
    "                        kernel_constraint= sigmoid_kernel_constraint,\n",
    "                        bias_initializer=Zeros,                        \n",
    "                        kernel_regularizer=sigmoid_kernel_regularisation,\n",
    "                        bias_regularizer=sigmoid_bias_regularisation\n",
    "                       )(phi_hyper)\n",
    "\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "        \n",
    "    # inject pre-trained embedding weights into Embedding layer\n",
    "    model.get_layer(name='TermEmbedding').set_weights([embeddings_matrix])\n",
    "    model.get_layer(name='TermEmbedding').trainable = train_embeddings    \n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The training algorithm incorporates mini-batch stochastic descent and negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(model,       # the model which parameters will be learnt\n",
    "          epochs,      # number of epochs to run          \n",
    "          batch_size,  # size of mini-batch\n",
    "          m,           # number of negative samples\n",
    "          data,        # data required for training                              \n",
    "          neg_strategy\n",
    "         ):\n",
    "\n",
    "    # create negative tuples\n",
    "    #negative_tuples = get_negative_tuples(data.train_query + data.test_query,\n",
    "     #                                     data.train_hyper + data.test_hyper, data.neg_vocab, m)\n",
    "    \n",
    "    print (\"Generating negative tuples...\")\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_strategy, m)\n",
    "    print (\"Negative tuples...ok\")\n",
    "    \n",
    "    # create sequences\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "                \n",
    "    samples = np.arange(len(term_train_seq))\n",
    "    validation_samples = np.arange(len(term_test_seq))\n",
    "    \n",
    "    # train algorithm\n",
    "    for epoch in range(epochs):\n",
    "        # reset loss\n",
    "        loss = 0.\n",
    "        test_loss = 0.\n",
    "                        \n",
    "        np.random.shuffle(samples)        \n",
    "\n",
    "        shuffled_X_term, shuffled_X_hyper =\\\n",
    "            np.array(term_train_seq, dtype='int32')[samples],\\\n",
    "            np.array(hyper_train_seq, dtype='int32')[samples]\n",
    "\n",
    "        for b in range(0, len(samples), batch_size):\n",
    "            # product mini-batch, consisting of 32 +ve samples\n",
    "            batch_X_term = shuffled_X_term[b:b + batch_size] \n",
    "            batch_X_hyper = shuffled_X_hyper[b:b + batch_size]\n",
    "\n",
    "            # complement +ve samples with negatives\n",
    "            batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "            extend_batch_with_negatives(batch_X_term, batch_X_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "            \n",
    "            # shuffle validation set indices\n",
    "            np.random.shuffle(validation_samples)\n",
    "            # pick batch of shuffled test instances with size equal to training batch\n",
    "            batch_X_test_term, batch_X_test_hyper =\\\n",
    "                np.array(term_test_seq, dtype='int32')[validation_samples[:batch_size]],\\\n",
    "                np.array(hyper_test_seq, dtype='int32')[validation_samples[:batch_size]]\n",
    "            \n",
    "            # distort test batch with some negatives to check how algorithm fares with\n",
    "            # negatives\n",
    "            batch_X_test_term, batch_X_test_hyper, batch_y_test_label =\\\n",
    "            extend_batch_with_negatives(batch_X_test_term, batch_X_test_hyper,\n",
    "                                        negative_tuples,\n",
    "                                        data.tokenizer\n",
    "                                       )            \n",
    "\n",
    "            # train on batch\n",
    "            loss += model.train_on_batch([batch_X_term, batch_X_hyper], \n",
    "                                          batch_y_label)[0]\n",
    "            \n",
    "            test_loss += model.test_on_batch([batch_X_test_term, batch_X_test_hyper], \n",
    "                                              batch_y_test_label)[0]                \n",
    "            \n",
    "        print('Epoch:', epoch+1, 'Loss:', loss, 'Test Loss:', test_loss)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  mix_hyper_synonym Phi Init:  random_identity Dropout:  False\n",
      "Generating negative tuples...\n",
      "Negative tuples...ok\n",
      "Epoch: 1 Loss: 57.722871869802475 Test Loss: 56.6726176738739\n",
      "Epoch: 2 Loss: 35.62424777448177 Test Loss: 35.56502993404865\n",
      "Epoch: 3 Loss: 29.496768444776535 Test Loss: 30.083672389388084\n",
      "Epoch: 4 Loss: 25.99754847586155 Test Loss: 26.973308578133583\n",
      "Epoch: 5 Loss: 23.48804147541523 Test Loss: 24.98801949620247\n",
      "Epoch: 6 Loss: 21.520301684737206 Test Loss: 23.68659245967865\n",
      "Epoch: 7 Loss: 19.911133125424385 Test Loss: 22.670902863144875\n",
      "Epoch: 8 Loss: 18.51924930512905 Test Loss: 22.519148021936417\n",
      "Epoch: 9 Loss: 17.283996485173702 Test Loss: 21.71220640093088\n",
      "Epoch: 10 Loss: 16.171499885618687 Test Loss: 21.015945971012115\n",
      "Epoch: 11 Loss: 15.155263021588326 Test Loss: 21.38067301362753\n",
      "Epoch: 12 Loss: 14.221636325120926 Test Loss: 21.29163869470358\n",
      "Epoch: 13 Loss: 13.360369879752398 Test Loss: 20.831284269690514\n",
      "Epoch: 14 Loss: 12.560510620474815 Test Loss: 21.761929519474506\n",
      "Epoch: 15 Loss: 11.79845455288887 Test Loss: 20.87448101490736\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "\n",
    "#rand_norm_m0_sd001 = RandomNormal(mean = 0.0, stddev=0.01, seed=42)\n",
    "#rand_norm = RandomNormal(mean = 0.0, stddev=1., seed=42)\n",
    "\n",
    "# negative sampling options\n",
    "neg_sampling_options = {'synonym':get_negative_words, \n",
    "                        'mix_hyper_synonym': mix_sim_hyper_random,\n",
    "                        'similar_hyponym': get_similar_hyponyms,\n",
    "                        'random': get_random_negative_words\n",
    "                       }\n",
    "\n",
    "# phi random init options\n",
    "phi_init_options = {'random_identity': random_identity, 'random_normal': random_normal}\n",
    "\n",
    "# implement mini-batch stochastic training\n",
    "epochs = 15\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# number of negative samples\n",
    "m = 10\n",
    "phi_k = 1\n",
    "train_embeddings = False\n",
    "negative_option = 'mix_hyper_synonym'\n",
    "phi_init_option = 'random_identity'\n",
    "do_dropout = False\n",
    "np.random.seed(42)\n",
    "\n",
    "# create model\n",
    "crim_model = get_CRIM_model(phi_k = phi_k, train_embeddings = train_embeddings,\n",
    "                            embeddings_dim = data.embeddings_dim, vocab_size = len(data.tokenizer.word_counts),\n",
    "                            embeddings_matrix = data.embedding_matrix,\n",
    "                            phi_init = phi_init_options[phi_init_option],                            \n",
    "                            sigmoid_kernel_regularisation = l2(0.001),\n",
    "                            sigmoid_bias_regularisation = l2(0.001),\n",
    "                            sigmoid_kernel_constraint = None,#ForceToOne(),\n",
    "                            do_dropout = do_dropout\n",
    "                           )\n",
    "\n",
    "print (\"Training started...\")\n",
    "print ('Epochs: ', epochs, 'Batch size: ', batch_size, 'm: ', m, 'pki_k: ', phi_k, 'train_embeddings: ', train_embeddings,\n",
    "      'Negative sampling: ', negative_option, 'Phi Init: ', phi_init_option, 'Dropout: ', do_dropout)\n",
    "\n",
    "train(crim_model, epochs, batch_size, m, data, neg_sampling_options[negative_option])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[2.9366813]], dtype=float32), array([-0.589812], dtype=float32)]\n",
      "0.0014073642\n"
     ]
    }
   ],
   "source": [
    "# have a look at the prediction layer weights\n",
    "print (crim_model.get_layer(name='Prediction').get_weights())\n",
    "#projs = ['Phi0', 'Phi1', 'Phi2', 'Phi3', 'Phi4']\n",
    "projs = ['Phi0']\n",
    "for p in projs:\n",
    "    print (np.mean(crim_model.get_layer(name=p).get_weights()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.training.Model at 0x7faaf4135cf8>"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Evaluation  code\n",
    "\n",
    "Main observations:<br>\n",
    "1. Tendency is for the model to overfit if we make the model larger than 1 projection matrix;\n",
    "1. Negative samples are important for the model to learn which words are not hypernyms;\n",
    "1. Although the model does seem to learn the correct words that are related to hypernymy to the query terms, it does not stop it from predicting with high confidence that similar but completely unrelated words are also hypernyms;\n",
    "    1. This is really apparent for animals where the model is not able to distinguish between vertebrate and invertebrate; mammal; animal; and so forth;\n",
    "    1. It's possible that we did not have enough examples to distinguish the various types of animals from each other;\n",
    "    1. Also, more targeted negative samples could have helped but these would have to be hand-created;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('object', 0.9918138),\n",
       " ('artefact', 0.9729182),\n",
       " ('furnishing', 0.9690847),\n",
       " ('artifact', 0.95833313),\n",
       " ('place', 0.9513724),\n",
       " ('material', 0.9458266),\n",
       " ('furniture', 0.9362237),\n",
       " ('structure', 0.92470324),\n",
       " ('durable', 0.90841484),\n",
       " ('activity', 0.90772265),\n",
       " ('property', 0.8954227),\n",
       " ('piece', 0.8928697),\n",
       " ('equipment', 0.8792076),\n",
       " ('event', 0.8615166),\n",
       " ('building', 0.8571182)]"
      ]
     },
     "execution_count": 1010,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "\n",
    "def crim_get_top_hypernyms(query, hyper_candidates, model, data, top):\n",
    "    query_index = data.tokenizer.word_index[query]    \n",
    "    valid_candidates = list(filter(lambda w: w != [query_index], hyper_candidates))\n",
    "    \n",
    "    candidate_sim = list(map(lambda x: model.predict([[query_index], x]).flatten()[0], valid_candidates))       \n",
    "    top_idx = np.argsort(candidate_sim)[::-1][:top]    \n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "    \n",
    "    return [(data.tokenizer.index_word[t], candidate_sim[top_idx[i]]) for i, t in enumerate(top_hyper)]\n",
    "\n",
    "\n",
    "crim_get_top_hypernyms('sofa', hyper_candidates, crim_model, data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.26786485]], dtype=float32)"
      ]
     },
     "execution_count": 976,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = data.tokenizer.word_index['pool']\n",
    "j = data.tokenizer.word_index['group']\n",
    "crim_model.predict([[i], [j]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### MRR, p@k evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_hypernyms_to_one_line(data):\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    one_line = {}\n",
    "    for w in ordered_queries:\n",
    "        word_hypernyms = [h for q, h in zip(data.test_query, data.test_hyper) if q == w]\n",
    "        one_line[w] = word_hypernyms\n",
    "    return one_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from task_scorer.py provided with shared task resources\n",
    "def mean_reciprocal_rank(r):\n",
    "    \"\"\"Score is reciprocal of the rank of the first relevant item\n",
    "    First element is 'rank 1'.  Relevance is binary (nonzero is relevant).\n",
    "    Example from http://en.wikipedia.org/wiki/Mean_reciprocal_rank\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Mean reciprocal rank\n",
    "    \"\"\"\n",
    "    r = np.asarray(r).nonzero()[0]\n",
    "    return 1. / (r[0] + 1) if r.size else 0.\n",
    "\n",
    "def precision_at_k(r, k, n):\n",
    "    \"\"\"Score is precision @ k\n",
    "    Relevance is binary (nonzero is relevant).\n",
    "    Args:\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "    Returns:\n",
    "        Precision @ k\n",
    "    Raises:\n",
    "        ValueError: len(r) must be >= k\n",
    "    \"\"\"\n",
    "    assert k >= 1\n",
    "    r = np.asarray(r)[:k] != 0\n",
    "    if r.size != k:\n",
    "        raise ValueError('Relevance score length < k')\n",
    "    return (np.mean(r)*k)/min(k,n)\n",
    "    # Modified from the first version. Now the gold elements are taken into account\n",
    "\n",
    "# used by Ustalov from https://github.com/nlpub/hyperstar/blob/master/evaluate.py\n",
    "def compute_ats(data, measures):\n",
    "    return [sum(measures[j].values()) / len(data.test_query) for j in range(len(measures))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predictions is a dictionary whereby key is query term and value is a list of ranked hypernym predictions\n",
    "def get_evaluation_scores(data, predictions):\n",
    "    all_scores = []    \n",
    "    scores_names = ['MRR', 'P@1', 'P@5', 'P@10']\n",
    "    for query, gold_hyps in convert_hypernyms_to_one_line(data).items():\n",
    "\n",
    "        avg_pat1 = []\n",
    "        avg_pat2 = []\n",
    "        avg_pat3 = []\n",
    "\n",
    "        pred_hyps = predictions[query]\n",
    "        gold_hyps_n = len(gold_hyps)    \n",
    "        r = [0 for i in range(15)]\n",
    "\n",
    "        for j in range(len(pred_hyps)):\n",
    "            if j < gold_hyps_n:\n",
    "                pred_hyp = pred_hyps[j]\n",
    "                if pred_hyp in gold_hyps:\n",
    "                    r[j] = 1\n",
    "\n",
    "        avg_pat1.append(precision_at_k(r,1,gold_hyps_n))\n",
    "        avg_pat2.append(precision_at_k(r,5,gold_hyps_n))\n",
    "        avg_pat3.append(precision_at_k(r,10,gold_hyps_n))    \n",
    "\n",
    "        mrr_score_numb = mean_reciprocal_rank(r)\n",
    "        avg_pat1_numb = sum(avg_pat1)/len(avg_pat1)\n",
    "        avg_pat2_numb = sum(avg_pat2)/len(avg_pat2)\n",
    "        avg_pat3_numb = sum(avg_pat3)/len(avg_pat3)\n",
    "\n",
    "        score_results = [mrr_score_numb, avg_pat1_numb, avg_pat2_numb, avg_pat3_numb]\n",
    "        all_scores.append(score_results)\n",
    "    return scores_names, all_scores\n",
    "\n",
    "def get_ustalov_evaluation_scores(data, predictions):\n",
    "    measures = [{} for _ in range(10)]\n",
    "\n",
    "    for i, (t,h) in enumerate(zip(data.test_query, data.test_hyper)):\n",
    "        actual = predictions[t]\n",
    "        for j in range(0, len(measures)):\n",
    "            measures[j][(t, h)] = 1. if h in actual[:j + 1] else 0.\n",
    "\n",
    "    ats = compute_ats(data, measures) \n",
    "    return ats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# alternative hypernym generator by applying Phi weights to hyponym and see which \n",
    "# words are closest to this vector\n",
    "def alt_get_hypernym(word, model, data, embeddings, top, bias = 0):\n",
    "    q_idx = data.tokenizer.word_index[word]    \n",
    "    \n",
    "    q = embeddings[q_idx]    \n",
    "        \n",
    "    try:\n",
    "        _phi = model.get_layer(name='Phi0').get_weights()[0]\n",
    "    except ValueError:\n",
    "        _phi = model.get_layer(name='Phi').get_weights()[0]\n",
    "        \n",
    "    #\n",
    "    #_phi = model.get_layer(name='Phi0').get_weights()[0] +\\\n",
    "    #       model.get_layer(name='Phi1').get_weights()[0] +\\\n",
    "    #       model.get_layer(name='Phi2').get_weights()[0]    \n",
    "    _proj = np.dot(q, _phi)\n",
    "    #_proj /= np.linalg.norm(_proj)\n",
    "    \n",
    "    #sim = cosine_similarity(embeddings[1:], _proj.reshape(1,-1)).flatten() \n",
    "    sim = np.array(list(map(lambda v: np.dot(v, _proj), embeddings[1:]))) + bias\n",
    "    \n",
    "    return list(map(lambda i: (data.tokenizer.index_word[i+1], sim[i]), np.argsort(sim)[::-1][:top]))\n",
    "\n",
    "def ustalov_get_hypernyms(word, _model, data, embeddings, top):\n",
    "    q_idx = data.tokenizer.word_index[word]        \n",
    "    q = embeddings[q_idx]       \n",
    "    \n",
    "    _phi = _model.get_layer(name='Phi0').get_weights()[0]\n",
    "        \n",
    "    Y_hat = np.dot(q, _phi)\n",
    "    Y_hat /= np.linalg.norm(Y_hat)    \n",
    "    \n",
    "    return model.similar_by_vector(Y_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cborg/jfarrugia/venv/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Ky.Busch_##-###', 0.5357945561408997),\n",
       " ('M.Kenseth_###-###', 0.5254930257797241),\n",
       " ('J.McMurray_###-###', 0.5196651816368103),\n",
       " ('E.Sadler_###-###', 0.5190649032592773),\n",
       " ('G.Biffle_###-###', 0.4628258943557739),\n",
       " ('Nasdaq_NASDAQ_TRIN', 0.4541865885257721),\n",
       " ('mso_para_margin_0in', 0.4513596296310425),\n",
       " ('T.Stewart_##-###', 0.44903600215911865),\n",
       " ('HuMax_IL8_TM', 0.43771377205848694),\n",
       " ('K.Kahne_###-###', 0.43192803859710693)]"
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ustalov_get_hypernyms('lime', crim_model, data, crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0], 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 0.8147533),\n",
       " ('vertebrate', 0.63432205),\n",
       " ('deaf', 0.4659867),\n",
       " ('human', 0.4530565),\n",
       " ('responsive', 0.43644807),\n",
       " ('neural', 0.33325133),\n",
       " ('communication', 0.17331588),\n",
       " ('relational', 0.16485256),\n",
       " ('animal', 0.14382486),\n",
       " ('patient', 0.11207277),\n",
       " ('competent', 0.09620102),\n",
       " ('placental', 0.08047774),\n",
       " ('emotion', 0.0673254),\n",
       " ('profession', 0.06708583),\n",
       " ('familial', 0.056225777)]"
      ]
     },
     "execution_count": 1321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test get hypernym routine\n",
    "alt_get_hypernym('deaf',crim_model, data, crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0], 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_crim_hypernyms(data, model):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    results = {}\n",
    "        \n",
    "    embeddings = crim_model.get_layer(name=\"TermEmbedding\").get_weights()[0]                     \n",
    "    for idx, word in enumerate(ordered_queries):        \n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "        #predicted_hypers = crim_get_top_hypernyms(word, hyper_candidates, model, data, 15)\n",
    "        predicted_hypers = alt_get_hypernym(word, model, data, embeddings, 15)\n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 25\n",
      "Done 50\n",
      "Done 75\n",
      "Done 100\n",
      "Done 125\n",
      "Done 150\n",
      "Done 175\n",
      "Done 200\n",
      "Done 225\n",
      "Done 250\n",
      "Done 275\n",
      "Done 300\n",
      "Done 325\n",
      "Done 350\n",
      "Done 375\n",
      "Done 400\n",
      "Done 425\n",
      "Done 450\n",
      "CRIM evaluation:\n",
      "MRR: 0.52341\n",
      "P@1: 0.5\n",
      "P@5: 0.37957\n",
      "P@10: 0.35763\n",
      "\n",
      "Ustalov-style evaluation:\n",
      "A@1=0.1468, A@2=0.2429, A@3=0.3240, A@4=0.3851, A@5=0.4409, A@6=0.4649, A@7=0.4916, A@8=0.5123, A@9=0.5305, A@10=0.5416\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "crim_predictions = predict_crim_hypernyms(data, crim_model)\n",
    "\n",
    "print (\"CRIM evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores(data, crim_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "print (\"\")\n",
    "print (\"Ustalov-style evaluation:\")\n",
    "ats = get_ustalov_evaluation_scores(data, crim_predictions)\n",
    "ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "print (ats_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Results\n",
    "* Epochs:  15 Batch size:  32 m:  5 pki_k:  1 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_normal\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.48746<br>\n",
    "P@1: 0.43584<br>\n",
    "P@5: 0.34417<br>\n",
    "P@10: 0.32088<br>\n",
    "\n",
    "* Epochs:  20 Batch size:  32 m:  5 pki_k:  1 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_normal\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.4903<br>\n",
    "P@1: 0.43584<br>\n",
    "P@5: 0.35612<br>\n",
    "P@10: 0.33251<br>\n",
    "\n",
    "* Epochs:  10 Batch size:  32 m:  5 pki_k:  5 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_normal\n",
    "    * LR parameters are unconstrained\n",
    "    \n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.49336<br>\n",
    "P@1: 0.45133<br>\n",
    "P@5: 0.36176<br>\n",
    "P@10: 0.34156<br>\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_normal\n",
    "    * LR parameters constrained\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.49779<br>\n",
    "P@1: 0.45133<br>\n",
    "P@5: 0.36622<br>\n",
    "P@10: 0.3412<br>\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_identity\n",
    "    * LR parameters constrained\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.49926<br>\n",
    "P@1: 0.45354<br>\n",
    "P@5: 0.36947<br>\n",
    "P@10: 0.34335<br>\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  5 pki_k:  1 train_embeddings:  False Negative sampling:  mix_hyper_synonym Phi Init:  random_identity\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.49819<br>\n",
    "P@1: 0.46239<br>\n",
    "P@5: 0.33621<br>\n",
    "P@10: 0.31726<br>\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  mix_hyper_synonym Phi Init:  random_identity\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.52017<br>\n",
    "P@1: 0.48894<br>\n",
    "P@5: 0.36892<br>\n",
    "P@10: 0.34381<br>\n",
    "\n",
    "Ustalov-style evaluation:<br>\n",
    "A@1=0.1435, A@2=0.2455, A@3=0.3253, A@4=0.3792, A@5=0.4162, A@6=0.4455, A@7=0.4695, A@8=0.4844, A@9=0.4981, A@10=0.5117<br>\n",
    "\n",
    "*With regularisation and no constraining of LR parameters*<br>\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.52341<br>\n",
    "P@1: 0.5<br>\n",
    "P@5: 0.37957<br>\n",
    "P@10: 0.35763<br>\n",
    "\n",
    "Ustalov-style evaluation:\n",
    "A@1=0.1468, A@2=0.2429, A@3=0.3240, A@4=0.3851, A@5=0.4409, A@6=0.4649, A@7=0.4916, A@8=0.5123, A@9=0.5305, A@10=0.5416<br>\n",
    "\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  synonym Phi Init:  random_identity Dropout:  True\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.47622<br>\n",
    "P@1: 0.43363<br>\n",
    "P@5: 0.32861<br>\n",
    "P@10: 0.3021<br>\n",
    "\n",
    "* Epochs:  15 Batch size:  32 m:  10 pki_k:  1 train_embeddings:  False Negative sampling:  random Phi Init:  random_identity Dropout:  False\n",
    "\n",
    "CRIM evaluation:<br>\n",
    "MRR: 0.49723<br>\n",
    "P@1: 0.44912<br>\n",
    "P@5: 0.37105<br>\n",
    "P@10: 0.35053<br>\n",
    "\n",
    "Ustalov evaluation:<br>\n",
    "A@1=0.1318, A@2=0.2390, A@3=0.3260, A@4=0.3968, A@5=0.4487, A@6=0.4857, A@7=0.5149, A@8=0.5448, A@9=0.5623,< A@10=0.5766<br>\n",
    "\n",
    "**Note how evaluation metrics paint a different picture of which algorithm fares best.  The random negative sampling model with unconstrained (but regularised) prediction layer parameters, beats the constrained model using a mix of similar hypernyms and random synonyms.**\n",
    "**The latter yields slightly more accuracy highly-ranked hypernyms but then has the tendency to suggest incorrect results with lesser probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Yamane Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## attempt custom constraint to keep weight fixed at 1.\n",
    "from tensorflow.keras.constraints import Constraint\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "class ForceToOne (Constraint):    \n",
    "    def __call__(self, w):\n",
    "        w /= w\n",
    "        return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Embedding, Dot, Flatten, Concatenate, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.initializers import RandomNormal, Zeros, Ones\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_new_cluster_model(embedding_layer, \n",
    "                          phi_dim = 300,\n",
    "                          phi_init = None,\n",
    "                          phi_activity_regularisation = None,                          \n",
    "                          sigmoid_bias_regularisation = None,\n",
    "                          sigmoid_kernel_constraint = None,\n",
    "                          do_dropout = False):\n",
    "    \n",
    "    hypo_input = Input(shape=(1,), name='Hyponym')    \n",
    "    hyper_input = Input(shape=(1,), name='Hypernym')\n",
    "    \n",
    "    hypo_embedding, hyper_embedding = embedding_layer([hypo_input, hyper_input])\n",
    "    \n",
    "    if do_dropout:\n",
    "        hypo_embedding = Dropout(0.25)(hypo_embedding)\n",
    "        hyper_embedding = Dropout(0.25)(hyper_embedding)\n",
    "                \n",
    "    phi = Dense(phi_dim, activation=None, use_bias=False,                 \n",
    "                kernel_initializer=phi_init,                \n",
    "                name='Phi0')(hypo_embedding)\n",
    "    \n",
    "    # flatten phi and hyper_embedding tensors\n",
    "    phi = Flatten()(phi)\n",
    "    hyper_embedding = Flatten()(hyper_embedding)\n",
    "    \n",
    "    phi_hyper = Dot(axes=-1, normalize=False, name='DotProduct')([phi, hyper_embedding])    \n",
    "    \n",
    "    predictions = Dense(1, activation = \"sigmoid\", \n",
    "                        bias_initializer = Zeros,\n",
    "                        kernel_initializer = Ones,\n",
    "                        kernel_constraint = sigmoid_kernel_constraint,                        \n",
    "                        bias_regularizer=sigmoid_bias_regularisation, \n",
    "                        name='Prediction')(phi_hyper)\n",
    "    # instantiate model\n",
    "    model = Model(inputs=[hypo_input, hyper_input], outputs=predictions)\n",
    "    \n",
    "    # compile using binary_crossentropy loss\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We don't need a unique embedding layer for every sub-model.  \n",
    "\n",
    "Instead, we can create a separate model for the embeddings and set the weights according to the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_embeddings_model(dim, embedding_matrix):\n",
    "    hypo_input = Input(shape=(1,))\n",
    "    hyper_input = Input(shape=(1,))\n",
    "\n",
    "    word_embedding = Embedding(embedding_matrix.shape[0], dim, name='WE')\n",
    "\n",
    "    hypo_embedding = word_embedding(hypo_input)\n",
    "    hyper_embedding = word_embedding(hyper_input)\n",
    "\n",
    "    embedding_model = Model(inputs=[hypo_input, hyper_input], outputs=[hypo_embedding, hyper_embedding])\n",
    "\n",
    "    # inject pre-trained embeddings into this mini, resusable model/layer\n",
    "    embedding_model.get_layer(name='WE').set_weights([embedding_matrix])\n",
    "    embedding_model.get_layer(name='WE').trainable = False\n",
    "    return embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class YamaneCluster:\n",
    "    def __init__(self, embedding_layer, phi_dim, phi_init, sigmoid_kernel_constraint):\n",
    "        \n",
    "        self.model = get_new_cluster_model(embedding_layer = embedding_layer, \n",
    "                                           phi_dim = phi_dim, \n",
    "                                           phi_init = phi_init, \n",
    "                                           sigmoid_kernel_constraint = sigmoid_kernel_constraint)\n",
    "        self.epoch_count = 0\n",
    "        self.loss = 0.\n",
    "        self.test_loss = 0.\n",
    "    \n",
    "    def increment_epoch(self):\n",
    "        self.epoch_count += 1\n",
    "        \n",
    "    def update_loss(self, new_loss):\n",
    "        self.loss += new_loss\n",
    "        \n",
    "    def update_test_loss(self, new_loss):\n",
    "        self.test_loss += new_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yamane_train(\n",
    "    epochs,      # number of epochs to run\n",
    "    m,           # number of negative samples\n",
    "    data,        # class instance containing all the data required for training/testing        \n",
    "    embedding_layer,\n",
    "    threshold    = 0.15,     # threshold; similarity below this score will trigger new cluster\n",
    "    negative_option = 'random', # inject lambda responsible for determining negative sample choice\n",
    "    phi_init_option = None,\n",
    "    sigmoid_kernel_constraint = None):    \n",
    "  \n",
    "    \n",
    "    neg_sampling_options = {'synonym':get_negative_words, \n",
    "                            'mix_hyper_synonym': mix_sim_hyper_random,\n",
    "                            'similar_hyponym': get_similar_hyponyms,\n",
    "                            'random': get_random_negative_words\n",
    "                           }\n",
    "    \n",
    "    phi_init_options = {'random_identity': random_identity, 'random_normal': random_normal}\n",
    "        \n",
    "    print (\"Generating negative tuples...\")\n",
    "    negative_tuples = get_negative_tuples((data.train_query + data.test_query, data.train_hyper + data.test_hyper), \n",
    "                                           data, neg_sampling_options[negative_option], m)\n",
    "    print (\"Negative tuples...ok\")\n",
    "    \n",
    "    # create sequences\n",
    "    # we have two sets of inputs: one for training query and hypernym terms;\n",
    "    #                             another for the validation query/hyper terms;\n",
    "    term_train_seq = data.tokenizer.texts_to_sequences(data.train_query)\n",
    "    hyper_train_seq = data.tokenizer.texts_to_sequences(data.train_hyper)\n",
    "\n",
    "    term_test_seq = data.tokenizer.texts_to_sequences(data.test_query)\n",
    "    hyper_test_seq = data.tokenizer.texts_to_sequences(data.test_hyper)\n",
    "    \n",
    "    # convert all to arrays\n",
    "    term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq =\\\n",
    "    [np.array(x, dtype='int32') for x in [term_train_seq, hyper_train_seq, term_test_seq, hyper_test_seq]]\n",
    "            \n",
    "    # this list stores which cluster each training sequence pertains to\n",
    "    sample_clusters = np.zeros(len(term_train_seq), dtype='int32')\n",
    "    \n",
    "    print (\"m: \", m, \"lambda: \", threshold, \"max epoch per cluster: \", epochs, \n",
    "           \"Negative sampling: \", negative_option, 'Phi Init: ', phi_init_option)\n",
    "    \n",
    "    \n",
    "    print (\"Sample clusters size: \", len(sample_clusters))\n",
    "    # list containing 1 model per cluster\n",
    "    clusters = []\n",
    "    # add default model to our list of models\n",
    "    # we share the embedding layer loaded with the pre-trained weights\n",
    "    # append tuple where 1st element is the cluster and 2nd element is the \n",
    "    # number of epochs that cluster is trained\n",
    "    \n",
    "    clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim,\n",
    "                                  phi_init = phi_init_options[phi_init_option], sigmoid_kernel_constraint = sigmoid_kernel_constraint))\n",
    "    \n",
    "    # get training set indices\n",
    "    indices = np.arange(len(term_train_seq))  \n",
    "    \n",
    "    # get test set indices\n",
    "    test_indices = np.arange(len(term_test_seq))\n",
    "            \n",
    "    # initialise each training sample to cluster 0\n",
    "    sample_clusters[indices] = 0        \n",
    "    \n",
    "    # seed random generator\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # indicator of \"current\" sample cluster index\n",
    "    z_i = 0\n",
    "    \n",
    "    # train algorithm\n",
    "    #for epoch in range(epochs):\n",
    "    epoch = 0\n",
    "    test_loss = 0.    \n",
    "    \n",
    "    while np.min([c.epoch_count for c in clusters]) < epochs:\n",
    "        # reset loss for each cluster                        \n",
    "        for c in clusters:\n",
    "            if c.epoch_count < epochs:                \n",
    "                c.loss = 0.\n",
    "            c.test_loss = 0.\n",
    "        \n",
    "        test_loss = 0.\n",
    "        \n",
    "        # shuffle indices every epoch\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # train algorithm by stochastic gradient descent, one sample at a time\n",
    "        for idx, i in enumerate(indices):                        \n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print (\"Processed \", idx+1, \"samples...\")\n",
    "            \n",
    "            # calculate similarity on all clusters\n",
    "            sim = list(map(lambda x: x.model.predict([term_train_seq[i], hyper_train_seq[i]]), clusters))\n",
    "            max_sim = np.argmax(sim)\n",
    "            #print \"Term:\", tokenizer.index_word[term_train_seq[i][0]], 'Hyper:', tokenizer.index_word[hyper_train_seq[i][0]], \"Max Similarity cluster:\", max_sim, \"(sim = %0.8f)\" % (sim[max_sim])\n",
    "            # limit cluster creation to a max of 25.\n",
    "            if ((sim[max_sim] < threshold) and (len(clusters) < 25)): \n",
    "                # add new cluster to list of clusters\n",
    "                clusters.append(YamaneCluster(embedding_layer, phi_dim=data.embeddings_dim,\n",
    "                                phi_init = phi_init_options[phi_init_option], sigmoid_kernel_constraint = sigmoid_kernel_constraint))\n",
    "                \n",
    "                # assign current cluster index to latest model\n",
    "                z_i = len(clusters) - 1\n",
    "                sample_clusters[i] = z_i\n",
    "            else:            \n",
    "                z_i = max_sim\n",
    "                sample_clusters[i] = z_i                \n",
    "                        \n",
    "            # if current cluster reached/exceeded epoch count, skip current sample (i.e don't update cluster)\n",
    "            if clusters[z_i].epoch_count < epochs:                                            \n",
    "                # extend samples in cluster with negative samples\n",
    "                batch_X_term, batch_X_hyper, batch_y_label =\\\n",
    "                    extend_batch_with_negatives(term_train_seq[i], \n",
    "                                                hyper_train_seq[i],\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "\n",
    "                # update parameters of cluster \n",
    "                clusters[z_i].update_loss(\n",
    "                    clusters[z_i].model.train_on_batch([batch_X_term, batch_X_hyper], batch_y_label)[0]\n",
    "                )\n",
    "            \n",
    "            # measure test loss \n",
    "            # every 32 samples (and updates are processed), we will test performance on validation set\n",
    "            # of 32 randomly chosen samples. We will record test loss of every cluster and report on \n",
    "            # lowest loss\n",
    "            \n",
    "            if (idx + 1) % 100 == 0:\n",
    "                np.random.shuffle(test_indices)\n",
    "                batch_query, batch_hyper = term_test_seq[test_indices[:32]], hyper_test_seq[test_indices[:32]]\n",
    "                batch_query, batch_hyper, test_y_label =\\\n",
    "                    extend_batch_with_negatives(batch_query, \n",
    "                                                batch_hyper,\n",
    "                                                negative_tuples,\n",
    "                                                data.tokenizer\n",
    "                                               )  \n",
    "                #batch_label = [1.] * batch_query.shape[0]\n",
    "                for q, h, l in zip(batch_query, batch_hyper, test_y_label):                                    \n",
    "                    test_losses = list(map(lambda c: c.model.test_on_batch([q, h], [l])[0], clusters))\n",
    "                    best_cluster = np.argmin(test_losses)\n",
    "                    clusters[best_cluster].update_test_loss(\n",
    "                        test_losses[best_cluster]\n",
    "                    )                    \n",
    "                                                                                                                      \n",
    "        # increase epoch count for clusters\n",
    "        for cluster in clusters:            \n",
    "            cluster.epoch_count += 1\n",
    "                \n",
    "        print('Epoch:', max([c.epoch_count for c in clusters]), 'Cluster #:', len(clusters) ,\n",
    "              'Loss:', np.mean([c.loss for c in clusters]),\n",
    "              'Test Loss:', np.mean([c.test_loss for c in clusters]))\n",
    "    return clusters, sample_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started...\n",
      "Generating negative tuples...\n",
      "Negative tuples...ok\n",
      "m:  10 lambda:  0.2 max epoch per cluster:  15 Negative sampling:  random Phi Init:  random_normal\n",
      "Sample clusters size:  4374\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "Epoch: 1 Cluster #: 19 Loss: 104.56347475024431 Test Loss: 223.83506843689094\n",
      "Processed  500 samples...\n",
      "Processed  1000 samples...\n",
      "Processed  1500 samples...\n",
      "Processed  2000 samples...\n",
      "Processed  2500 samples...\n",
      "Processed  3000 samples...\n",
      "Processed  3500 samples...\n",
      "Processed  4000 samples...\n",
      "Epoch: 2 Cluster #: 25 Loss: 55.59035038062371 Test Loss: 86.60853448438691\n",
      "Processed  500 samples...\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "# initialise embedding later which will be shared among all clusters\n",
    "embedding_layer = get_embeddings_model(dim=data.embeddings_dim, embedding_matrix=data.embedding_matrix)\n",
    "epochs = 15\n",
    "m = 10\n",
    "\n",
    "print (\"Training started...\")\n",
    "clusters, sample_clusters =\\\n",
    "    yamane_train(epochs, m, \n",
    "                 data,\n",
    "                 embedding_layer,\n",
    "                 threshold = 0.2,\n",
    "                 negative_option = 'random',\n",
    "                 phi_init_option = 'random_normal',\n",
    "                 sigmoid_kernel_constraint = ForceToOne())\n",
    "\n",
    "print (datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yamane Clusters\n",
    "\n",
    "Analysing the hypernym element of the training pairs consistuting a cluster, a pattern emerges. The training algorithm has realised a semantic split and has clustered terms around particular hypernyms. \n",
    "Give a model trained on the following parameters:\n",
    "* m:  10 lambda:  0.15 max epoch per cluster:  15 Negative sampling:  random Phi Init:  random_normal\n",
    "\n",
    ", the most populated clusters where in descending order: \n",
    "* Counter({4: 743, 0: 674, 7: 311, 1: 279, 5: 236, 13: 166, 11: 149, 22: 139, 23: 138, 6: 135, 24: 118, 21: 117, 18: 116, 19: 108, 17: 106, 3: 105, 20: 97, 14: 94, 16: 90, 15: 89, 9: 80, 10: 78, 12: 72, 8: 68, 2: 66})\n",
    "\n",
    "For instance, cluster 4, the most populated clusters, features animals (282), mammals (119) and birds (50).  Cluster 0 (second most populated), features chordates (203), vertebrates(201), placentals (108) and invertebrates (56). <br>\n",
    "\n",
    "It is likely that the model is learning the optimal projections to transform any given hyponym to the hypernym which features frequently in a cluster.  For instance if we multiply the embedding of\"barley\" with projection matrix learnt from the samples in cluster 0, the resultant vector would be similar to \"vertebrate\" even though barley is clearly not an animal.  \n",
    "\n",
    "When we compute the product of the projection matric in every cluster and compare the result with every word in the vocab, the top 15 most similar words are:\n",
    "\n",
    "[('plant', 9.292432),<br>\n",
    " ('food', 5.7546735),<br>\n",
    " ('herb', 5.4948673),<br>\n",
    " ('vertebrate', 5.256508),<br>\n",
    " ('chordate', 4.754751),<br>\n",
    " ('cereal', 4.3708754),<br>\n",
    " ('placental', 4.155033),<br>\n",
    " ('grass', 4.0776873),<br>\n",
    " ('factory', 3.7920942),<br>\n",
    " ('beverage', 3.3642926),<br>\n",
    " ('shrub', 3.3227572),<br>\n",
    " ('animal', 3.1966455),<br>\n",
    " ('produce', 2.8567104),<br>\n",
    " ('ruminant', 2.2009413),<br>\n",
    " ('angiosperm', 2.1797898)]<br>\n",
    "\n",
    "Turns out that 5 (out of 6)  of the hypernyms in the gold test data were correctly generated.  However note that chordate, vertebrate and placental features in the top 10 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('action', 2),\n",
       " ('state', 2),\n",
       " ('space', 2),\n",
       " ('education', 2),\n",
       " ('light', 2),\n",
       " ('gray', 2),\n",
       " ('aid', 2),\n",
       " ('number', 1),\n",
       " ('religion', 1),\n",
       " ('brief', 1),\n",
       " ('unspecified', 1),\n",
       " ('process', 1),\n",
       " ('representational', 1),\n",
       " ('rear', 1),\n",
       " ('low', 1),\n",
       " ('flash', 1),\n",
       " ('vocation', 1),\n",
       " ('payment', 1),\n",
       " ('settlement', 1),\n",
       " ('finite', 1),\n",
       " ('direction', 1),\n",
       " ('mental', 1),\n",
       " ('country', 1),\n",
       " ('affect', 1),\n",
       " ('brochure', 1),\n",
       " ('pamphlet', 1),\n",
       " ('uncontrollable', 1),\n",
       " ('flower', 1),\n",
       " ('event', 1),\n",
       " ('art', 1),\n",
       " ('growth', 1),\n",
       " ('grow', 1),\n",
       " ('scale', 1),\n",
       " ('data', 1),\n",
       " ('hit', 1),\n",
       " ('act', 1),\n",
       " ('approach', 1),\n",
       " ('machine', 1),\n",
       " ('crystal', 1),\n",
       " ('labor', 1),\n",
       " ('relax', 1),\n",
       " ('soil', 1),\n",
       " ('man', 1),\n",
       " ('direct', 1),\n",
       " ('container', 1),\n",
       " ('part', 1),\n",
       " ('group', 1),\n",
       " ('explode', 1),\n",
       " ('muscle', 1),\n",
       " ('grub', 1),\n",
       " ('shade', 1),\n",
       " ('possible', 1),\n",
       " ('ethnic', 1),\n",
       " ('church', 1),\n",
       " ('abandoned', 1),\n",
       " ('government', 1),\n",
       " ('wake', 1),\n",
       " ('sleep', 1),\n",
       " ('structural', 1),\n",
       " ('computer', 1),\n",
       " ('school', 1),\n",
       " ('college', 1),\n",
       " ('affected', 1),\n",
       " ('system', 1),\n",
       " ('live', 1),\n",
       " ('divide', 1),\n",
       " ('film', 1),\n",
       " ('city', 1),\n",
       " ('operate', 1),\n",
       " ('business', 1),\n",
       " ('heath', 1),\n",
       " ('airship', 1),\n",
       " ('description', 1),\n",
       " ('wave', 1),\n",
       " ('revolving', 1),\n",
       " ('pool', 1),\n",
       " ('coin', 1),\n",
       " ('record', 1),\n",
       " ('flow', 1),\n",
       " ('commerce', 1),\n",
       " ('road', 1),\n",
       " ('demand', 1)]"
      ]
     },
     "execution_count": 1299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypers_in_cluster = list(map(lambda x: data.train_hyper[x], np.where(sample_clusters == 15)[0]))\n",
    "freq_hyper_cluster = Counter(hypers_in_cluster)\n",
    "sorted(freq_hyper_cluster.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({4: 743, 0: 674, 7: 311, 1: 279, 5: 236, 13: 166, 11: 149, 22: 139, 23: 138, 6: 135, 24: 118, 21: 117, 18: 116, 19: 108, 17: 106, 3: 105, 20: 97, 14: 94, 16: 90, 15: 89, 9: 80, 10: 78, 12: 72, 8: 68, 2: 66})\n",
      "------------------------------\n",
      "Train and test loss per cluster\n",
      "0 18 2.077968140053599 26.584612346770555\n",
      "1 18 0.14291605495270687 2.4012529607851434\n",
      "2 18 3.011270795017481 15.81435315310955\n",
      "3 18 1.3532882147701457 22.07756668852926\n",
      "4 18 3.5027269545994386 26.21724240878588\n",
      "5 18 0.9470082458719844 20.615928533309784\n",
      "6 18 0.6829920894379029 9.659515039220878\n",
      "7 18 3.2296777088486124 13.683177031301511\n",
      "8 18 4.120160838589072 36.14920896291733\n",
      "9 18 6.023580802604556 28.57362263975665\n",
      "10 18 4.613474638201296 55.94250735640526\n",
      "11 17 1.688921358785592 4.030947924022257\n",
      "12 17 4.820179348811507 44.305954933166504\n",
      "13 17 3.86210466385819 27.297351107948998\n",
      "14 17 2.7912403827067465 18.074579687789083\n",
      "15 17 6.939839515835047 62.952078223228455\n",
      "16 17 4.9971641432493925 30.62264429219067\n",
      "17 17 5.283988178707659 29.750021874904633\n",
      "18 16 6.731314154341817 34.248991971835494\n",
      "19 16 5.975748836994171 31.562748543918133\n",
      "20 16 4.89718824904412 13.97224909067154\n",
      "21 16 5.415210111998022 47.26156715862453\n",
      "22 16 5.557033606804907 29.347868665587157\n",
      "23 16 5.251510926987976 73.7665657151374\n",
      "24 15 6.579021139070392 31.6229175850749\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# show distribution of samples over the trained clusters\n",
    "print (Counter(sample_clusters))\n",
    "print (\"-\"*30)\n",
    "print (\"Train and test loss per cluster\")\n",
    "\n",
    "for idx, c in enumerate(clusters):\n",
    "    print (idx, c.epoch_count, c.loss, c.test_loss)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yamane_get_top_hypernym(query, hyper_candidates, clusters, data, top):    \n",
    "    query_index = data.tokenizer.word_index[query]\n",
    "    # remove actual query from candidates    \n",
    "    valid_candidates = list(filter(lambda x: x[0]!=query_index, hyper_candidates))\n",
    "    hyper_probs = []\n",
    "    for idx, hyper in enumerate(valid_candidates):    \n",
    "        if (idx+1) % 500 == 0:\n",
    "            print (\"Done\", idx+1)\n",
    "        candidate_sim = list(map(lambda x: x.model.predict([[query_index], hyper]).flatten()[0], clusters))\n",
    "        hyper_probs.append(np.max(candidate_sim))\n",
    "    \n",
    "    top_idx = np.argsort(hyper_probs)[::-1][:top]\n",
    "    top_hyper = np.array(valid_candidates)[top_idx].flatten()\n",
    "            \n",
    "    return [(data.tokenizer.index_word[t], hyper_probs[top_idx[i]]) for i, t in enumerate(top_hyper)]\n",
    "\n",
    "\n",
    "def alt_yamane_get_top(word, hyper_candidates, clusters, data, top, bias_list):\n",
    "    \n",
    "    yam_results = []\n",
    "    for idx, c in enumerate(clusters):\n",
    "        yam_results.extend(\\\n",
    "            alt_get_hypernym(word, c.model, data, embedding_layer.get_layer(name=\"WE\").get_weights()[0], 10, bias=yamane_bias[idx])\n",
    "                          )\n",
    "    return sorted(yam_results, key= lambda x:x[1], reverse=True)[:top]\n",
    "\n",
    "def predict_yamane_hypernyms(data, clusters):\n",
    "    hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "    \n",
    "    # store biases in list\n",
    "    yamane_bias = list(map(lambda c: c.model.get_layer('Prediction').get_weights()[1][0], clusters))\n",
    "    \n",
    "    ordered_queries = sorted(list(set(data.test_query)))\n",
    "    \n",
    "    results = {}\n",
    "    for idx, word in enumerate(ordered_queries):\n",
    "        if (idx + 1) % 25 == 0:\n",
    "            print (\"Done\", idx + 1)\n",
    "        \n",
    "        # find clusters best suited to this word\n",
    "        word_id = data.tokenizer.word_index[word]\n",
    "        \n",
    "        cluster_probs = neigh.predict_proba(data.embedding_matrix[word_id].reshape(1,-1))\n",
    "        cluster_idx = np.where(cluster_probs > 0.)[1]\n",
    "        #print (cluster_idx)\n",
    "        specific_clusters = map(lambda c: clusters[c], cluster_idx)\n",
    "        specific_bias = map(lambda c: yamane_bias[c], cluster_idx)\n",
    "\n",
    "        predicted_hypers = alt_yamane_get_top(word, hyper_candidates, specific_clusters, data, 15, specific_bias )\n",
    "        \n",
    "        \n",
    "        #predicted_hypers = yamane_get_top_hypernym(word, hyper_candidates, clusters, data, 15)\n",
    "        #predicted_hypers = alt_yamane_get_top(word, hyper_candidates, clusters, data, 15, yamane_bias)        \n",
    "        \n",
    "        results[word] = [h for h, p in predicted_hypers]\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('person', 4.9793377),\n",
       " ('animal', 2.47742),\n",
       " ('human', 1.8638403),\n",
       " ('placental', 1.6627152),\n",
       " ('instrument', 1.5909408),\n",
       " ('chordate', 1.2495062),\n",
       " ('move', 1.1529647),\n",
       " ('vertebrate', 1.0523603),\n",
       " ('device', 1.0224875),\n",
       " ('employee', 1.0139179),\n",
       " ('mammal', 0.95096517),\n",
       " ('primate', 0.9028685),\n",
       " ('plant', 0.75226915),\n",
       " ('place', 0.63126254),\n",
       " ('change', 0.5526115)]"
      ]
     },
     "execution_count": 1322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yamane_predictions = predict_yamane_hypernyms(data, clusters)\n",
    "hyper_candidates = [[data.tokenizer.word_index[hyper]] for hyper in data.tokenizer.word_index.keys()]\n",
    "yamane_bias = list(map(lambda c: c.model.get_layer('Prediction').get_weights()[1][0], clusters))\n",
    "alt_yamane_get_top('deaf', hyper_candidates, clusters, data, 15, yamane_bias )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 500\n",
      "Done 1000\n",
      "Done 1500\n",
      "Done 2000\n",
      "Done 2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('plant', 0.9999012),\n",
       " ('vertebrate', 0.99234927),\n",
       " ('food', 0.9917059),\n",
       " ('herb', 0.9899386),\n",
       " ('invertebrate', 0.9893778),\n",
       " ('chordate', 0.980608),\n",
       " ('angiosperm', 0.9798537),\n",
       " ('factory', 0.9751135),\n",
       " ('tree', 0.96472),\n",
       " ('produce', 0.961811),\n",
       " ('object', 0.9617095),\n",
       " ('animal', 0.9588083),\n",
       " ('mollusk', 0.9435754),\n",
       " ('shrub', 0.93360406),\n",
       " ('placental', 0.92714185)]"
      ]
     },
     "execution_count": 933,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yamane_get_top_hypernym('lime', hyper_candidates, clusters, data, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1317,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 25\n",
      "Done 50\n",
      "Done 75\n",
      "Done 100\n",
      "Done 125\n",
      "Done 150\n",
      "Done 175\n",
      "Done 200\n",
      "Done 225\n",
      "Done 250\n",
      "Done 275\n",
      "Done 300\n",
      "Done 325\n",
      "Done 350\n",
      "Done 375\n",
      "Done 400\n",
      "Done 425\n",
      "Done 450\n",
      "Yamane evaluation:\n",
      "MRR: 0.49281\n",
      "P@1: 0.46018\n",
      "P@5: 0.36722\n",
      "P@10: 0.34988\n",
      "\n",
      "Ustalov-style evaluation:\n",
      "A@1=0.1351, A@2=0.2286, A@3=0.3156, A@4=0.3734, A@5=0.4273, A@6=0.4604, A@7=0.4864, A@8=0.5091, A@9=0.5208, A@10=0.5351\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "yamane_predictions = predict_yamane_hypernyms(data, clusters)\n",
    "\n",
    "print (\"Yamane evaluation:\")\n",
    "score_names, all_scores = get_evaluation_scores(data, yamane_predictions)\n",
    "for k in range(len(score_names)):\n",
    "    print (score_names[k]+': '+str(round(sum([score_list[k] for score_list in all_scores]) / len(all_scores), 5)))\n",
    "print (\"\")\n",
    "print (\"Ustalov-style evaluation:\")\n",
    "ats = get_ustalov_evaluation_scores(data, yamane_predictions)\n",
    "ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "print (ats_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['instrument',\n",
       " 'move',\n",
       " 'device',\n",
       " 'change',\n",
       " 'system',\n",
       " 'build',\n",
       " 'step',\n",
       " 'join',\n",
       " 'expand',\n",
       " 'push',\n",
       " 'receptive',\n",
       " 'departure',\n",
       " 'positive',\n",
       " 'organ',\n",
       " 'tool']"
      ]
     },
     "execution_count": 1323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yamane_predictions['deaf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yamane evaluation\n",
    "\n",
    "* m:  5 lambda:  0.15 max epoch per cluster:  13 Negative sampling:  mix_hyper_synonym Phi Init:  random_identity\n",
    "\n",
    "Yamane evaluation:<br>\n",
    "MRR: 0.45324<br>\n",
    "P@1: 0.4292<br>\n",
    "P@5: 0.33544<br>\n",
    "P@10: 0.30875<br>\n",
    "\n",
    "Ustalov-style evaluation:<br>\n",
    "A@1=0.1260, A@2=0.2188, A@3=0.3013, A@4=0.3669, A@5=0.4273, A@6=0.4526, A@7=0.4883, A@8=0.5052, A@9=0.5221, A@10=0.5305<br>\n",
    "\n",
    "* m:  1 lambda:  0.15 max epoch per cluster:  13 Negative sampling:  random Phi Init:  random_normal\n",
    "\n",
    "Yamane evaluation:<br>\n",
    "MRR: 0.46375<br>\n",
    "P@1: 0.4292<br>\n",
    "P@5: 0.31766<br>\n",
    "P@10: 0.29952<br>\n",
    "\n",
    "Ustalov-style evaluation:<br>\n",
    "A@1=0.1260, A@2=0.2195, A@3=0.2825, A@4=0.3416, A@5=0.3877, A@6=0.4247, A@7=0.4558, A@8=0.4838, A@9=0.5039, A@10=0.5195<br>\n",
    "\n",
    "* m:  10 lambda:  0.15 max epoch per cluster:  15 Negative sampling:  random Phi Init:  random_normal\n",
    "\n",
    "Yamane evaluation:<br>\n",
    "MRR: 0.45739<br>\n",
    "P@1: 0.41593<br>\n",
    "P@5: 0.32338<br>\n",
    "P@10: 0.30494<br>\n",
    "\n",
    "Ustalov-style evaluation:<br>\n",
    "A@1=0.1221, A@2=0.2058, A@3=0.2825, A@4=0.3442, A@5=0.3922, A@6=0.4312, A@7=0.4565, A@8=0.4851, A@9=0.5097, A@10=0.5286<br>\n",
    "\n",
    "Employed novel method that makes better use learnt clusters. Rather than getting closest words according to projection in every cluster (and then filtering to leave the best 15), I trained a KNN classifier which attempts to find the better suited cluster for the term in question.  Reduces obvious wrong words since certain clusters have learnt a projection matrix that would transform several words to the strongest hypernym featuring in the cluster.\n",
    "\n",
    "Yamane evaluation:<br>\n",
    "MRR: 0.49281<br>\n",
    "P@1: 0.46018<br>\n",
    "P@5: 0.36722<br>\n",
    "P@10: 0.34988<br>\n",
    "\n",
    "Ustalov-style evaluation:<br>\n",
    "A@1=0.1351, A@2=0.2286, A@3=0.3156, A@4=0.3734, A@5=0.4273, A@6=0.4604, A@7=0.4864, A@8=0.5091, A@9=0.5208, A@10=0.5351<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[4.969965e-05]], dtype=float32),\n",
       " array([[0.00042733]], dtype=float32),\n",
       " array([[0.01260389]], dtype=float32),\n",
       " array([[0.00849416]], dtype=float32),\n",
       " array([[0.01360245]], dtype=float32),\n",
       " array([[0.0467734]], dtype=float32),\n",
       " array([[0.0040016]], dtype=float32),\n",
       " array([[0.00175099]], dtype=float32),\n",
       " array([[0.02694884]], dtype=float32),\n",
       " array([[0.0825713]], dtype=float32),\n",
       " array([[0.04480027]], dtype=float32),\n",
       " array([[0.00564445]], dtype=float32),\n",
       " array([[0.13343635]], dtype=float32),\n",
       " array([[0.00391912]], dtype=float32),\n",
       " array([[0.02489026]], dtype=float32),\n",
       " array([[0.0522318]], dtype=float32),\n",
       " array([[0.02140403]], dtype=float32),\n",
       " array([[0.0131434]], dtype=float32),\n",
       " array([[0.01463544]], dtype=float32),\n",
       " array([[0.01401767]], dtype=float32),\n",
       " array([[0.92166936]], dtype=float32),\n",
       " array([[0.00787069]], dtype=float32),\n",
       " array([[0.02718614]], dtype=float32),\n",
       " array([[0.00640392]], dtype=float32),\n",
       " array([[0.01362688]], dtype=float32)]"
      ]
     },
     "execution_count": 1155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = data.tokenizer.word_index['baby']\n",
    "j = data.tokenizer.word_index['child']\n",
    "list(map(lambda c: c.model.predict([[i], [j]]),  clusters))\n",
    "\n",
    "#plant = .99990785 (1)\n",
    "#vertebrate = .99481356 (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch Pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('journey', 0.50752056),\n",
       " ('fun', 0.4609611),\n",
       " ('trip', 0.45013088),\n",
       " ('ride', 0.38148546),\n",
       " ('movie', 0.3800118),\n",
       " ('magical', 0.3711656),\n",
       " ('drama', 0.3643443),\n",
       " ('vacation', 0.36418122),\n",
       " ('genre', 0.36171854),\n",
       " ('hero', 0.34951678),\n",
       " ('romantic', 0.34861058),\n",
       " ('action', 0.34297746),\n",
       " ('comedy', 0.34139755),\n",
       " ('hobby', 0.34065622),\n",
       " ('tour', 0.33641037),\n",
       " ('warrior', 0.33598843),\n",
       " ('dream', 0.3273697),\n",
       " ('canoe', 0.32696596),\n",
       " ('entertainment', 0.32566392),\n",
       " ('experience', 0.32389012)]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to given word\n",
    "word = data.tokenizer.word_index['adventure']\n",
    "candidate_words = list(filter(lambda w: w != word, data.tokenizer.index_word.keys()))\n",
    "sims = list(map(lambda c: np.dot(data.embedding_matrix[c], data.embedding_matrix[word]), candidate_words))\n",
    "\n",
    "most_sim_idx = np.argsort(sims)[::-1][:20]\n",
    "#print most_sim_idx\n",
    "\n",
    "[(data.tokenizer.index_word[candidate_words[idx]], sims[idx]) for idx in most_sim_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1063,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('flatworm', 0.5927298),\n",
       " ('mammal', 0.588327),\n",
       " ('species', 0.5788616),\n",
       " ('ichthyosaur', 0.57654685),\n",
       " ('arthropod', 0.5764982),\n",
       " ('hominid', 0.5761199),\n",
       " ('invertebrate', 0.5743749),\n",
       " ('archosaur', 0.5668075),\n",
       " ('monotreme', 0.5665382),\n",
       " ('morphological', 0.5580014),\n",
       " ('angiosperm', 0.5572169),\n",
       " ('chordate', 0.54813623),\n",
       " ('trilobite', 0.5478228),\n",
       " ('pterosaur', 0.5452015),\n",
       " ('organism', 0.5425958),\n",
       " ('herbivore', 0.53324604),\n",
       " ('gastropod', 0.52406955),\n",
       " ('annelid', 0.51789314),\n",
       " ('crinoid', 0.51332396),\n",
       " ('planarian', 0.51214486)]"
      ]
     },
     "execution_count": 1063,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find most similar words to given word\n",
    "word = data.tokenizer.word_index['vertebrate']\n",
    "candidate_words = list(filter(lambda w: w != word, data.tokenizer.index_word.keys()))\n",
    "tuned_embeddings = crim_model.get_layer(name='TermEmbedding').get_weights()[0]\n",
    "\n",
    "sims = list(map(lambda c: np.dot(tuned_embeddings[c], tuned_embeddings[word]), candidate_words))\n",
    "\n",
    "most_sim_idx = np.argsort(sims)[::-1][:20]\n",
    "#print most_sim_idx\n",
    "\n",
    "[(data.tokenizer.index_word[candidate_words[idx]], sims[idx]) for idx in most_sim_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cborg/jfarrugia/venv/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('vertebrates', 0.7684643268585205),\n",
       " ('mammalian', 0.7008777856826782),\n",
       " ('mammals', 0.6720753908157349),\n",
       " ('chordates', 0.6348909139633179),\n",
       " ('placental_mammals', 0.6302692294120789),\n",
       " ('terrestrial_vertebrates', 0.624941349029541),\n",
       " ('monotremes', 0.6244850754737854),\n",
       " ('crocodilians', 0.6234016418457031),\n",
       " ('vertebrate_evolution', 0.6229584217071533),\n",
       " ('arthropods', 0.6225826144218445),\n",
       " ('vertebrate_animals', 0.6223456859588623),\n",
       " ('metazoans', 0.6213091015815735),\n",
       " ('herbivorous_dinosaurs', 0.621167004108429),\n",
       " ('eukaryote', 0.6205124258995056),\n",
       " ('metazoan', 0.6191608309745789),\n",
       " ('theropod', 0.6179888844490051),\n",
       " ('Acanthostega', 0.6108344793319702),\n",
       " ('multicellular', 0.6099957823753357),\n",
       " ('Platynereis', 0.6092511415481567),\n",
       " ('multicellular_organisms', 0.6080698370933533)]"
      ]
     },
     "execution_count": 1062,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('vertebrate', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/cborg/jfarrugia/venv/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('dog', 0.9999999403953552),\n",
       " ('dogs', 0.8680489659309387),\n",
       " ('puppy', 0.8106428980827332),\n",
       " ('pit_bull', 0.7803961038589478),\n",
       " ('pooch', 0.7627376914024353),\n",
       " ('cat', 0.7609456777572632),\n",
       " ('golden_retriever', 0.7500901818275452),\n",
       " ('German_shepherd', 0.7465174198150635),\n",
       " ('Rottweiler', 0.7437615394592285),\n",
       " ('beagle', 0.7418621182441711)]"
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model.vectors[2043])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01719806 -0.00749344 -0.05798202  0.05405104 -0.02833585  0.01924545\n",
      "  0.01965492 -0.02768068 -0.00515942 -0.02129283]\n",
      "[ 0.01719806 -0.00749344 -0.05798202  0.05405104 -0.02833585  0.01924545\n",
      "  0.01965492 -0.02768068 -0.00515942 -0.02129283]\n"
     ]
    }
   ],
   "source": [
    "model.vocab['dog'].index\n",
    "\n",
    "print (model.vectors[2043][:10])\n",
    "\n",
    "data.tokenizer.word_index['dog']\n",
    "print (data.embedding_matrix[50][:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A@1=0.1318, A@2=0.2390, A@3=0.3260, A@4=0.3968, A@5=0.4487, A@6=0.4857, A@7=0.5149, A@8=0.5448, A@9=0.5623, A@10=0.5766\n"
     ]
    }
   ],
   "source": [
    "measures = [{} for _ in range(10)]\n",
    "\n",
    "for i, (t,h) in enumerate(zip(data.test_query, data.test_hyper)):\n",
    "    actual = crim_predictions[t]\n",
    "    for j in range(0, len(measures)):\n",
    "        measures[j][(t, h)] = 1. if h in actual[:j + 1] else 0.\n",
    "\n",
    "ats = compute_ats(data, measures) \n",
    "ats_string = ', '.join(['A@%d=%.4f' % (j + 1, ats[j]) for j in range(len(ats))])\n",
    "print (ats_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#measures[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11405238127979088"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "sigmoid(-2.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "cat\n",
      "dog\n",
      "feline\n",
      "puppy\n",
      "pup\n",
      "pet\n",
      "chihuahua\n",
      "poodle\n",
      "rabbit\n",
      "raccoon\n",
      "rottweiler\n",
      "squirrel\n",
      "hamster\n",
      "animal\n",
      "fox\n"
     ]
    }
   ],
   "source": [
    "print (data.tokenizer.word_index['cat'])\n",
    "cat = data.embedding_matrix[27]\n",
    "\n",
    "for i in (np.argsort(list(map(lambda v: np.dot(v, cat), data.embedding_matrix[1:])))[::-1][:15] + 1):\n",
    "    print (data.tokenizer.index_word[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.1402805,\n",
       " -1.616902,\n",
       " -1.0481057,\n",
       " -1.237945,\n",
       " -2.1051261,\n",
       " -2.102034,\n",
       " -1.2530707,\n",
       " -1.7784206,\n",
       " -1.0288585,\n",
       " -1.04917,\n",
       " -1.0502187,\n",
       " -1.6933552,\n",
       " -1.0537158,\n",
       " -1.5516595,\n",
       " -1.218102,\n",
       " -1.1043793,\n",
       " -1.1490285,\n",
       " -1.2724837,\n",
       " -1.3093464,\n",
       " -1.300655,\n",
       " -1.2495532,\n",
       " -1.4313546,\n",
       " -1.4133878,\n",
       " -1.4823934,\n",
       " -1.4082193]"
      ]
     },
     "execution_count": 1016,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda c: c.model.get_layer('Prediction').get_weights()[1][0], clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create KNN dataset\n",
    "#set(map(lambda i: data.train_query[i], np.where(sample_clusters == 0)[0])\n",
    "corn_cluster_idx = set(sample_clusters[np.where(np.array(data.train_query) == 'deaf')[0]])\n",
    "print (corn_cluster_idx)\n",
    "specific_clusters = map(lambda c: clusters[c], corn_cluster_idx)\n",
    "specific_bias = map(lambda c: yamane_bias[c], corn_cluster_idx)\n",
    "\n",
    "alt_yamane_get_top('deaf', hyper_candidates, specific_clusters, data, 15, specific_bias )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare knn dataset based on learnt clusters\n",
    "# we will attempt to allocate \n",
    "train_seq = np.array(data.tokenizer.texts_to_sequences(train_query))\n",
    "\n",
    "X_knn = {}\n",
    "for idx, c in enumerate(clusters):\n",
    "    cluster_ids = np.where(sample_clusters == idx)\n",
    "    # we can reduce duplicate terms to unique terms    \n",
    "    uniq_terms = np.unique(train_seq[cluster_ids])\n",
    "    #print (uniq_terms)    \n",
    "    X_knn[idx] = data.embedding_matrix[uniq_terms]  \n",
    "\n",
    "X_features = X_knn[0]\n",
    "y = np.zeros(X_knn[0].shape[0], dtype='int16')\n",
    "\n",
    "for k in range(1,len(clusters)):\n",
    "    X_features = np.vstack((X_features, X_knn[k]))\n",
    "    y = np.hstack((y, np.array([k] * X_knn[k].shape[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='distance')"
      ]
     },
     "execution_count": 1297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
    "neigh.fit(X_features, y) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  4 11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('animal', 7.718166),\n",
       " ('vertebrate', 7.3052816),\n",
       " ('chordate', 6.3819933),\n",
       " ('placental', 5.169263),\n",
       " ('mammal', 4.9130435),\n",
       " ('device', 3.7047691),\n",
       " ('object', 3.5397835),\n",
       " ('carnivore', 3.4969392),\n",
       " ('feline', 3.0213947),\n",
       " ('canine', 2.9958434),\n",
       " ('primate', 2.7636704),\n",
       " ('reptile', 2.6483884),\n",
       " ('rodent', 2.5337505),\n",
       " ('bird', 2.5030375),\n",
       " ('creature', 2.3609676)]"
      ]
     },
     "execution_count": 1326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_id = data.tokenizer.word_index['mouse']\n",
    "\n",
    "cluster_probs = neigh.predict_proba(data.embedding_matrix[word_id].reshape(1,-1))\n",
    "cluster_idx = np.where(cluster_probs > 0.)[1]\n",
    "print (cluster_idx)\n",
    "specific_clusters = map(lambda c: clusters[c], cluster_idx)\n",
    "specific_bias = map(lambda c: yamane_bias[c], cluster_idx)\n",
    "\n",
    "alt_yamane_get_top('mouse', hyper_candidates, specific_clusters, data, 15, specific_bias )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 1280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
